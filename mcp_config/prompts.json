{
  "prompts": [
    {
      "name": "analyze_project",
      "description": "Comprehensive project analysis including data flow validation, component dependencies, performance assessment, security evaluation, and architecture review based on actual project verification logic",
      "arguments": [
        {
          "name": "project_id",
          "description": "ID of the project to analyze",
          "required": true
        },
        {
          "name": "analysis_depth",
          "description": "Analysis depth: 'basic' (config validation), 'detailed' (performance analysis), 'comprehensive' (full security and optimization review)",
          "required": false
        },
        {
          "name": "focus_areas",
          "description": "Specific areas to focus on: 'performance', 'security', 'reliability', 'scalability', or 'all'",
          "required": false
        }
      ],
      "template": "You are an expert security architect for AgentSmith-HUB SDPP. Your task is to conduct a thorough project analysis.\n\n## **Analysis Request**\n- **Project ID:** %s\n- **Analysis Depth:** %s\n- **Focus Areas:** %s\n\n---\n\n# üìä Project Analysis Guide (Based on Actual System Architecture)\n\n## **CRITICAL: Understanding Project Verification and Architecture**\nBased on detailed source code analysis (project.Verify(), parseContent(), component validation), these are the key analysis areas:\n\n## **Step 0: Data Context Analysis (Essential First Step)**\nBefore analyzing configuration, understand the data being processed:\n\n```bash\n# Get sample data from project inputs\nuse get_samplers_data with project_id\n\n# Understand data schema and volume\nuse get_qps_stats for throughput metrics\n\n# Check historical data patterns\nuse get_daily_messages for trend analysis\n```\n\n## **Step 1: Project Configuration Validation**\n\n### **Project Details Analysis:**\n- **Project ID:** %s\n- **Current Status:** %s\n- **Architecture:** Security Data Processing Pipeline\n\n**Component Dependencies:**\n%s\n\n**Project Configuration (YAML):**\n```yaml\n%s\n```\n%s\n\n### **Configuration Validation Checklist:**\n\n**Data Flow Syntax Analysis (parseContent() Logic):**\n- [ ] Content field not empty\n- [ ] Valid arrow syntax: `COMPONENT_TYPE.ID -> COMPONENT_TYPE.ID`\n- [ ] Component type validation: INPUT, OUTPUT, RULESET only\n- [ ] All referenced components exist (not just in .new files)\n- [ ] No circular dependencies in flow graph\n\n**Component Dependency Validation:**\n- [ ] All inputs exist and are configured properly\n- [ ] All outputs exist and have proper destinations\n- [ ] All rulesets exist and are syntactically valid\n- [ ] Component compatibility (data schema alignment)\n\n## **Step 2: Architecture and Data Flow Analysis**\n\n### **Data Flow Patterns:**\n```bash\n# Analyze flow complexity\nuse get_project with project_id for flow graph\n\n# Check component health\nuse get_inputs, get_outputs, get_rulesets\n\n# Verify component connectivity\nuse verify_component for each referenced component\n```\n\n**Flow Pattern Assessment:**\n- **Simple Flow:** INPUT ‚Üí RULESET ‚Üí OUTPUT\n- **Branched Flow:** INPUT ‚Üí RULESET ‚Üí (OUTPUT1, OUTPUT2)\n- **Merged Flow:** (INPUT1, INPUT2) ‚Üí RULESET ‚Üí OUTPUT\n- **Complex Flow:** Multiple branches and merges\n\n### **Component Health Check:**\n- **Input Components:** Connectivity, authentication, topic availability\n- **Ruleset Components:** XML validation, plugin availability, performance\n- **Output Components:** Destination connectivity, batch configuration, permissions\n\n## **Step 3: Performance and Scalability Analysis**\n\n### **Throughput Analysis:**\n```bash\n# Current performance metrics\nuse get_system_stats for resource utilization\nuse get_qps_stats for component-level QPS\n\n# Component-specific performance\nuse get_component_qps for detailed metrics\n```\n\n**Performance Assessment Framework:**\n\n**Input Performance:**\n- Consumer group efficiency\n- Connection pooling optimization\n- Batch size configuration\n- Compression settings\n\n**Ruleset Performance:**\n- Rule complexity and filter efficiency\n- Plugin execution overhead\n- Threshold detection resource usage\n- Regular expression optimization\n\n**Output Performance:**\n- Batch size vs latency tradeoff\n- Destination connection pooling\n- Index/topic optimization\n- Compression and serialization\n\n### **Bottleneck Identification:**\n- **CPU Bound:** Complex regex, plugin execution\n- **Memory Bound:** Large batch sizes, threshold caching\n- **I/O Bound:** Network connectivity, disk writes\n- **Concurrency Bound:** Channel buffer sizes\n\n## **Step 4: Security Analysis**\n\n### **Data Security Assessment:**\n```bash\n# Search for security configurations\nuse search_components with 'auth', 'token', 'credential'\n\n# Check connection security\nuse search_components with 'ssl', 'tls', 'sasl'\n```\n\n**Security Validation Framework:**\n\n**Authentication & Authorization:**\n- [ ] Input authentication (SASL, API keys)\n- [ ] Output authentication (ES auth, Kafka SASL)\n- [ ] Secure credential storage (no plaintext)\n- [ ] Certificate validation for TLS\n\n**Data Protection:**\n- [ ] Encryption in transit (TLS/SSL)\n- [ ] Sensitive data masking in logs\n- [ ] PII handling compliance\n- [ ] Data retention policies\n\n**Network Security:**\n- [ ] Firewall configuration\n- [ ] Network segmentation\n- [ ] Internal vs external connections\n- [ ] Certificate management\n\n### **Threat Modeling:**\n- **Data Injection:** Malformed input validation\n- **Privilege Escalation:** Component permission boundaries\n- **Data Leakage:** Output destination security\n- **Denial of Service:** Resource exhaustion protection\n\n## **Step 5: Reliability and Operational Excellence**\n\n### **Reliability Assessment:**\n```bash\n# Check project health over time\nuse get_project_metrics with historical data\n\n# Monitor error rates\nuse get_error_logs for component failures\n```\n\n**Reliability Framework:**\n\n**Fault Tolerance:**\n- Component failure handling\n- Graceful degradation strategies\n- Error recovery mechanisms\n- Circuit breaker patterns\n\n**Monitoring & Alerting:**\n- QPS monitoring and thresholds\n- Error rate tracking\n- Resource utilization alerts\n- Component health checks\n\n**Operational Procedures:**\n- Deployment processes (3-step workflow)\n- Configuration change management\n- Rollback procedures\n- Incident response\n\n## **Step 6: Optimization Recommendations**\n\n### **Priority Matrix:**\n\n**Critical (Immediate Action Required):**\n- Security vulnerabilities\n- Configuration errors causing failures\n- Resource exhaustion risks\n- Data loss scenarios\n\n**High (Next Sprint):**\n- Performance bottlenecks\n- Scalability constraints\n- Monitoring gaps\n- Configuration optimization\n\n**Medium (Next Quarter):**\n- Architecture improvements\n- Additional security hardening\n- Advanced monitoring\n- Documentation updates\n\n**Low (Future Considerations):**\n- Feature enhancements\n- Technology upgrades\n- Process improvements\n- Training needs\n\n### **Implementation Roadmap:**\n\nFor each recommendation:\n1. **Impact Assessment:** Quantify benefits\n2. **Risk Analysis:** Identify potential issues\n3. **Implementation Plan:** Step-by-step approach\n4. **Validation Criteria:** Success metrics\n5. **Rollback Plan:** Failure recovery\n\n## **Step 7: Compliance and Best Practices**\n\n### **Security Compliance:**\n- Industry standards (SOC2, ISO27001)\n- Data protection regulations (GDPR, HIPAA)\n- Internal security policies\n- Audit trail requirements\n\n### **Operational Best Practices:**\n- Configuration as code\n- Automated testing\n- Change management\n- Documentation standards\n\nNow provide specific, data-driven analysis and recommendations based on the evidence gathered from the project inspection."
    },
    {
      "name": "debug_component",
      "description": "Advanced component debugging with root cause analysis based on actual verification logic and common failure patterns",
      "arguments": [
        {
          "name": "component_type",
          "description": "Type of component (input, output, plugin, ruleset, project)",
          "required": true
        },
        {
          "name": "component_id",
          "description": "ID of the component",
          "required": true
        },
        {
          "name": "issue_description",
          "description": "Description of the issue or error message",
          "required": false
        },
        {
          "name": "debug_level",
          "description": "Debug level: 'quick' (basic checks), 'detailed' (comprehensive analysis), 'deep' (system-level debugging)",
          "required": false
        }
      ],
      "template": "You are an expert security engineer for AgentSmith-HUB SDPP. Your task is to debug a component using systematic analysis.\n\n## **Debug Request Analysis**\n- **Component Type:** %s\n- **Component ID:** %s\n- **Issue Description:** %s\n- **Debug Level:** %s\n\n---\n\n# üîç Component Debugging Guide (Based on Actual Verification Logic)\n\n## **CRITICAL: Understanding Component-Specific Failure Patterns**\nBased on detailed source code analysis, these are the common failure patterns:\n\n### **Input Component Issues (input.Verify()):**\n**Configuration Errors:**\n- Missing required fields: `type`, `kafka.brokers`, `kafka.topic`\n- Invalid type values (must be exactly 'kafka' or 'aliyun_sls')\n- Empty arrays: `kafka.brokers` cannot be empty\n- Empty strings: `kafka.topic` cannot be empty\n\n**Runtime Errors:**\n- Connection failures: Broker unreachable, authentication failed\n- Topic not found: Invalid topic name or permissions\n- Consumer group conflicts: Multiple consumers with same group\n\n### **Output Component Issues (output.Verify()):**\n**Configuration Errors:**\n- Missing required fields: `type`, `kafka.brokers`, `kafka.topic`, `elasticsearch.hosts`, `elasticsearch.index`\n- Invalid type values (must be 'kafka', 'elasticsearch', 'print', 'aliyun_sls')\n- Batch configuration issues: invalid `batch_size` or `flush_dur`\n\n**Runtime Errors:**\n- Destination unreachable: ES cluster down, Kafka brokers offline\n- Authentication failures: Invalid credentials\n- Index/topic creation failures: Permissions or syntax errors\n\n### **Ruleset Component Issues (ValidateWithDetails()):**\n**XML Syntax Errors:**\n- Missing required attributes: `author`, `type`, `id`\n- Duplicate rule IDs within same ruleset\n- Invalid node types (not in 23 supported types)\n- Unbalanced XML tags or malformed structure\n\n**Logic Errors:**\n- Special characters without CDATA: `<`, `>`, `&` in content\n- Invalid threshold configuration: missing `group_by` or `range`\n- Plugin references to non-existent plugins\n- Invalid regex patterns in REGEX nodes\n\n### **Plugin Component Issues (plugin.Verify()):**\n**Code Syntax Errors:**\n- Invalid Go syntax: compilation failures\n- Wrong package name (must be 'plugin')\n- Missing Eval function\n\n**Signature Errors:**\n- Wrong return count: must be 2 or 3 values\n- Wrong return types: must be (bool, error) or (interface{}, bool, error)\n- External imports: only standard library allowed\n\n### **Project Component Issues (project.Verify()):**\n**Configuration Errors:**\n- Empty content field\n- Invalid data flow syntax: wrong arrow operator or component references\n- Missing component dependencies: referenced components don't exist\n\n**Runtime Errors:**\n- Component initialization failures\n- Circular dependencies in data flow\n- Channel connection failures\n\n## **Systematic Debugging Process:**\n\n### **Step 1: Gather Evidence**\n```bash\n# Check component existence and basic info\nuse get_[component_type] with component_id\n\n# Check usage and dependencies\nuse get_component_usage with component_id\n\n# Check pending changes\nuse get_pending_changes to see if component has modifications\n\n# For projects: check all component dependencies\nuse get_projects, get_inputs, get_outputs, get_rulesets\n```\n\n### **Step 2: Validation Analysis**\n```bash\n# Test component configuration\nuse verify_component with type and id\n\n# For connectivity issues\nuse test_[component_type]_connectivity if available\n\n# Check system resources\nuse get_system_stats\n```\n\n### **Step 3: Diagnosis Framework**\n\n**Configuration Issues (Most Common):**\n1. Run `verify_component` - check for line-specific errors\n2. Compare against validation requirements above\n3. Check for recent changes in pending files\n\n**Runtime Issues:**\n1. Check project status using `get_project`\n2. Review connectivity using component-specific test tools\n3. Check resource utilization and system health\n\n**Integration Issues:**\n1. Verify all dependency components exist\n2. Check data flow syntax in projects\n3. Validate component compatibility\n\n### **Step 4: Resolution Actions**\n\n**For Configuration Fixes:**\n1. Use `update_[component_type]` with corrected configuration\n2. Use `verify_component` to confirm fix\n3. Use `apply_single_change` to activate\n4. Use `restart_project` for affected projects\n\n**For Runtime Fixes:**\n1. Address connectivity/resource issues\n2. Restart affected projects using `restart_project`\n3. Monitor using `get_project` and system metrics\n\n## **Component-Specific Debugging Checklists:**\n\n### **Input Debugging:**\n- [ ] Valid type and required fields present\n- [ ] Broker/endpoint connectivity\n- [ ] Topic/logstore exists and accessible\n- [ ] Authentication credentials valid\n- [ ] No consumer group conflicts\n\n### **Output Debugging:**\n- [ ] Valid type and required fields present\n- [ ] Destination system online and accessible\n- [ ] Index/topic creation permissions\n- [ ] Batch configuration appropriate for data volume\n- [ ] No write permission issues\n\n### **Ruleset Debugging:**\n- [ ] XML well-formed and valid\n- [ ] All node types supported\n- [ ] Plugin references valid\n- [ ] Regex patterns compile\n- [ ] Threshold configuration complete\n- [ ] No duplicate rule IDs\n\n### **Plugin Debugging:**\n- [ ] Valid Go syntax\n- [ ] Correct function signature\n- [ ] Package name is 'plugin'\n- [ ] Only standard library imports\n- [ ] Eval function present\n\n### **Project Debugging:**\n- [ ] Content field not empty\n- [ ] Data flow syntax correct\n- [ ] All referenced components exist\n- [ ] No circular dependencies\n- [ ] Component types match references\n\nNow provide specific diagnosis and resolution steps based on the gathered evidence."
    },
    {
      "name": "optimize_performance",
      "description": "Data-driven performance optimization analysis and recommendations.",
      "arguments": [
        {
          "name": "focus_area",
          "description": "Optional: Area to focus on (e.g., 'throughput', 'cpu', 'memory')",
          "required": false
        }
      ],
      "template": "You are a Site Reliability Engineer (SRE) for AgentSmith-HUB, an SDPP (Security Data Pipeline Platform). Your task is to perform a data-driven performance analysis.\n\n## **Standard Operating Procedure (SOP) for Performance Optimization**\n\n### **Step 1: Gather Key Metrics (Required)**\n- Use the `get_system_stats` tool to get current CPU/Memory/Disk usage.\n- Use the `get_qps_stats` tool to understand the current request throughput.\n- Use `get_daily_messages` to understand historical data volume trends.\n\n### **Step 2: Analyze and Recommend**\nBased on the **data gathered in Step 1**, provide a performance analysis and concrete optimization recommendations. Focus on:\n- **Resource Bottlenecks:** Is CPU, memory, or I/O a limiting factor?\n- **Throughput Issues:** Is the QPS lower than expected or showing negative trends?\n- **Prioritized Actions:** Provide a list of actions, ordered by potential impact, to improve performance.\n\n(This template is a guide; the actual data from tools should drive the final analysis)"
    },
    {
      "name": "create_input_guide",
      "description": "Interactive guide for creating input components with comprehensive validation and configuration assistance based on input.Verify() and InputConfig structs",
      "arguments": [
        {
          "name": "input_type",
          "description": "Type of input to create: 'kafka' or 'aliyun_sls'",
          "required": true
        },
        {
          "name": "input_id",
          "description": "Unique identifier for the input component",
          "required": true
        },
        {
          "name": "assist_level",
          "description": "Level of assistance: 'basic' (template), 'guided' (step-by-step), 'advanced' (with validation)",
          "required": false
        }
      ],
      "template": "You are an expert DevOps engineer for AgentSmith-HUB SDPP. Your task is to create a valid input component configuration.\n\n## **Input Requirements Analysis**\n- **Input Type:** %s\n- **Input ID:** %s\n- **Assist Level:** %s\n\n---\n\n# üì• Input Component Creation Guide (Based on Actual Verification Logic)\n\n## **CRITICAL: Understanding Input.Verify() Requirements**\nBased on the source code analysis, these are the EXACT validation requirements:\n\n### **Required Fields (ALL TYPES):**\n1. **`type`** - Must be exactly 'kafka' or 'aliyun_sls' (case-sensitive)\n\n### **For Kafka Inputs (type: kafka):**\n**REQUIRED:**\n- `kafka.brokers` - Array of broker addresses (cannot be empty)\n- `kafka.topic` - Topic name (cannot be empty string)\n\n**OPTIONAL:**\n- `kafka.group` - Consumer group (recommended for production)\n- `kafka.compression` - Values: 'none', 'gzip', 'snappy', 'lz4', 'zstd'\n- `kafka.sasl` - SASL authentication config\n\n### **For Aliyun SLS Inputs (type: aliyun_sls):**\n**REQUIRED:**\n- `aliyun_sls.endpoint` - SLS endpoint URL\n- `aliyun_sls.access_key_id` - Access key\n- `aliyun_sls.access_key_secret` - Secret key\n- `aliyun_sls.project` - SLS project name\n- `aliyun_sls.logstore` - Logstore name\n- `aliyun_sls.consumer_group_name` - Consumer group\n- `aliyun_sls.consumer_name` - Consumer name\n\n**OPTIONAL:**\n- `aliyun_sls.cursor_position` - 'begin', 'end', or timestamp\n- `aliyun_sls.cursor_start_time` - Unix timestamp in milliseconds\n- `aliyun_sls.query` - Optional query for filtering\n\n## **Step-by-Step Creation Process:**\n\n1. **Verify Component Doesn't Exist**: Use `get_input` to check if ID is available\n2. **Create Configuration**: Use `create_input` with proper YAML\n3. **Test Configuration**: Use `verify_component` to validate\n4. **Apply Changes**: Follow the 3-step workflow (update ‚Üí apply ‚Üí restart)\n\nNow generate the complete and valid YAML configuration."
    },
    {
      "name": "create_output_guide", 
      "description": "Interactive guide for creating output components with comprehensive validation based on output.Verify() and OutputConfig structs",
      "arguments": [
        {
          "name": "output_type",
          "description": "Type of output: 'kafka', 'elasticsearch', 'print', or 'aliyun_sls'",
          "required": true
        },
        {
          "name": "output_id", 
          "description": "Unique identifier for the output component",
          "required": true
        },
        {
          "name": "assist_level",
          "description": "Level of assistance: 'basic' (template), 'guided' (step-by-step), 'advanced' (with validation)",
          "required": false
        }
      ],
      "template": "You are an expert DevOps engineer for AgentSmith-HUB SDPP. Your task is to create a valid output component configuration.\n\n## **Output Requirements Analysis**\n- **Output Type:** %s\n- **Output ID:** %s\n- **Assist Level:** %s\n\n---\n\n# üì§ Output Component Creation Guide (Based on Actual Verification Logic)\n\n## **CRITICAL: Understanding Output.Verify() Requirements**\nBased on the source code analysis, these are the EXACT validation requirements:\n\n### **Required Fields (ALL TYPES):**\n1. **`type`** - Must be exactly 'kafka', 'elasticsearch', 'print', or 'aliyun_sls'\n\n### **For Kafka Outputs (type: kafka):**\n**REQUIRED:**\n- `kafka.brokers` - Array of broker addresses (cannot be empty)\n- `kafka.topic` - Topic name (cannot be empty string)\n\n**OPTIONAL:**\n- `kafka.compression` - Values: 'none', 'gzip', 'snappy', 'lz4', 'zstd'\n- `kafka.sasl` - SASL authentication config\n- `kafka.key` - Message key template\n\n### **For Elasticsearch Outputs (type: elasticsearch):**\n**REQUIRED:**\n- `elasticsearch.hosts` - Array of ES hosts (cannot be empty)\n- `elasticsearch.index` - Index name (cannot be empty string)\n\n**OPTIONAL:**\n- `elasticsearch.batch_size` - Batch size for bulk operations (default: 100)\n- `elasticsearch.flush_dur` - Flush duration (e.g., '3s', '1m')\n- `elasticsearch.auth` - Authentication config\n\n### **For Print Outputs (type: print):**\n- No additional fields required (logs to console)\n\n### **For Aliyun SLS Outputs (type: aliyun_sls):**\n**REQUIRED:**\n- `aliyun_sls.endpoint`, `access_key_id`, `access_key_secret`, `project`, `logstore`\n\n## **Performance Considerations:**\n- **Batch Size**: For high-volume data, tune elasticsearch.batch_size\n- **Flush Duration**: Balance latency vs throughput\n- **Connection Pooling**: Multiple hosts for load balancing\n\nNow generate the complete and valid YAML configuration."
    },
    {
      "name": "create_project_guide",
      "description": "Comprehensive guide for creating data flow projects with detailed validation and dependency management based on project.Verify() and parseContent()",
      "arguments": [
        {
          "name": "project_id",
          "description": "Unique identifier for the project",
          "required": true
        },
        {
          "name": "flow_type",
          "description": "Type of data flow: 'simple' (linear), 'branched' (one-to-many), 'merged' (many-to-one), 'complex' (branched+merged)",
          "required": false
        },
        {
          "name": "use_case",
          "description": "Primary goal of this project (e.g., 'Process firewall logs for threat detection')",
          "required": false
        }
      ],
      "template": "You are an expert security architect for AgentSmith-HUB SDPP. Your task is to create a valid project configuration.\n\n## **Project Requirements Analysis**\n- **Project ID:** %s\n- **Flow Type:** %s\n- **Use Case:** %s\n\n---\n\n# üèóÔ∏è Project Creation Guide (Based on Actual parseContent() Logic)\n\n## **CRITICAL: Understanding Project.Verify() and parseContent() Requirements**\nBased on source code analysis, these are the EXACT validation requirements:\n\n### **Required Fields:**\n1. **`content`** - Cannot be empty or just whitespace\n\n### **Data Flow Syntax (parseContent() Logic):**\n**Valid Format:** `COMPONENT_TYPE.ID -> COMPONENT_TYPE.ID`\n\n**Component Types:**\n- `INPUT.id` - Must reference existing input component\n- `OUTPUT.id` - Must reference existing output component  \n- `RULESET.id` - Must reference existing ruleset component\n\n**Flow Examples:**\n- Simple: `INPUT.kafka_logs -> RULESET.security_rules -> OUTPUT.es_alerts`\n- Branched: `INPUT.logs -> RULESET.filter -> OUTPUT.es_storage, OUTPUT.kafka_alerts`\n- Merged: `INPUT.syslog, INPUT.app_logs -> RULESET.correlation -> OUTPUT.siem`\n- Complex: Multiple branches with different processing paths\n\n### **Component Dependency Validation:**\nThe system validates that:\n1. All referenced components exist (not just in temp .new files)\n2. No circular dependencies\n3. All inputs have downstream connections\n4. All outputs have upstream connections\n\n## **Step-by-Step Creation Process:**\n\n### **Step 1: Component Availability Check**\nUse these tools to verify components exist:\n- `get_inputs` - List available input components\n- `get_outputs` - List available output components  \n- `get_rulesets` - List available ruleset components\n\n### **Step 2: Design Data Flow**\nBased on available components from Step 1:\n- Map your use case to component chain\n- Consider branching for multiple outputs\n- Plan for data transformation needs\n\n### **Step 3: Create and Validate**\n1. Use `create_project` with proper YAML\n2. Use `verify_component` to validate syntax and dependencies\n3. Check for component existence errors\n\n### **Available Components:**\n%s\n%s\n%s\n\nNow generate the complete and valid project YAML configuration with proper data flow syntax."
    },
    {
      "name": "create_ruleset_guide",
      "description": "Extremely comprehensive guide for creating security rulesets with complete XML syntax validation based on ValidateWithDetails() and engine structs",
      "arguments": [
        {
          "name": "ruleset_type",
          "description": "Type of ruleset: 'DETECTION' (for threat detection) or 'WHITELIST' (for filtering)",
          "required": true
        },
        {
          "name": "ruleset_id",
          "description": "Unique identifier for the ruleset component",
          "required": true
        },
        {
          "name": "security_scenario",
          "description": "Security scenario: 'process_monitoring', 'network_analysis', 'file_integrity', 'user_behavior', 'malware_detection', or 'custom'",
          "required": false
        },
        {
          "name": "complexity_level", 
          "description": "Complexity: 'simple' (filter+checklist), 'intermediate' (with threshold), 'advanced' (complex conditions and plugins)",
          "required": false
        }
      ],
      "template": "You are an expert security analyst for AgentSmith-HUB SDPP. Your task is to create a valid ruleset XML configuration.\n\n## **Ruleset Requirements Analysis**\n- **Ruleset Type:** %s\n- **Ruleset ID:** %s\n- **Security Scenario:** %s\n- **Complexity Level:** %s\n\n---\n\n# üõ°Ô∏è Ruleset Creation Guide (Based on Actual ValidateWithDetails() Logic)\n\n## **CRITICAL: Understanding XML Validation Requirements**\nBased on detailed source code analysis, these are the EXACT validation rules:\n\n### **Root Element Requirements:**\n```xml\n<root author=\"your_name\" type=\"DETECTION|WHITELIST\">\n```\n- **`author`** - Required attribute\n- **`type`** - Must be exactly 'DETECTION' or 'WHITELIST'\n\n### **Rule Element Requirements:**\n```xml\n<rule id=\"unique_rule_id\" name=\"descriptive_name\">\n```\n- **`id`** - Required, must be unique across all rules\n- **`name`** - Optional but recommended\n- **No duplicate rule IDs allowed**\n\n### **Filter Element (Performance Critical):**\n```xml\n<filter field=\"field.path\">value_to_match</filter>\n```\n- **`field`** - Required, supports nested paths (e.g., 'data.process.name')\n- **Content** - Value to filter on (empty content triggers warning)\n\n### **Checklist Element (Core Logic):**\n```xml\n<checklist condition=\"(node1 AND node2) OR node3\">\n  <node id=\"node1\" type=\"REGEX\" field=\"process.name\">.*powershell.*</node>\n  <node id=\"node2\" type=\"INCL\" field=\"cmdline\" logic=\"OR\" delimiter=\"|\">-enc|-encoded</node>\n  <node id=\"node3\" type=\"PLUGIN\" field=\"source_ip\">is_internal_ip(_$source_ip)</node>\n</checklist>\n```\n\n### **23 Supported Node Types (validateCheckNodeType()):**\n1. **REGEX** - Regular expression matching\n2. **INCL** - Inclusion check (substring)\n3. **EXCL** - Exclusion check  \n4. **PLUGIN** - Custom plugin execution\n5. **EQ** - Exact equality\n6. **NE** - Not equal\n7. **GT** - Greater than\n8. **GE** - Greater than or equal\n9. **LT** - Less than\n10. **LE** - Less than or equal\n11. **RANGE** - Numeric range check\n12. **NULL** - Null/empty check\n13. **NOTNULL** - Not null check\n14. **PREFIX** - Prefix matching\n15. **SUFFIX** - Suffix matching\n16. **CONTAIN** - Contains check\n17. **NOTCONTAIN** - Does not contain\n18. **STARTSWITH** - Starts with\n19. **ENDSWITH** - Ends with\n20. **IN** - Value in list\n21. **NOTIN** - Value not in list\n22. **BETWEEN** - Between two values\n23. **JSONPATH** - JSON path extraction\n\n### **CDATA Usage (Critical for Special Characters):**\n```xml\n<node type=\"REGEX\" field=\"command\"><![CDATA[\\$[a-zA-Z]+\\s*=\\s*.*]]></node>\n```\n**MUST use CDATA when content contains:** `<`, `>`, `&`, quotes, or complex regex\n\n### **Threshold Element (Stateful Detection):**\n```xml\n<threshold group_by=\"source_ip\" range=\"300\" count_type=\"SUM\" local_cache=\"true\">5</threshold>\n```\n- **`group_by`** - Required field to group by\n- **`range`** - Required time range in seconds\n- **`count_type`** - 'SUM' or 'CLASSIFY'\n- **`local_cache`** - Performance optimization (true/false)\n\n### **Data Modification Elements:**\n```xml\n<append field=\"threat_level\">HIGH</append>\n<append type=\"PLUGIN\" field=\"geo_info\">get_geolocation(_$source_ip)</append>\n<del>temp_field,debug_info</del>\n<plugin>send_alert(_$ORIDATA, \"critical\")</plugin>\n```\n\n## **Validation Process:**\n1. **Create XML**: Use `create_ruleset` with proper XML\n2. **Validate**: Use `verify_component` - returns line numbers for errors\n3. **Check Plugin Availability**: Use `get_plugins` for PLUGIN nodes\n4. **Apply**: Follow 3-step workflow (update ‚Üí apply ‚Üí restart)\n\n## **Common Validation Errors to Avoid:**\n- Missing required attributes (author, type, id)\n- Duplicate rule IDs\n- Invalid node types\n- Unbalanced XML tags\n- Special characters without CDATA\n- Invalid threshold configuration\n\nNow generate the complete and valid XML ruleset for the specified requirements."
    },
    {
      "name": "guide_ruleset_plugins",
      "description": "A detailed guide on how to correctly use plugins within a Ruleset, covering all use cases.",
      "arguments": [
        {
          "name": "goal",
          "description": "Describe what you want to achieve with the plugin. (e.g., 'Check if a domain is on a blocklist', 'Add geolocation data for an IP')",
          "required": true
        }
      ],
      "template": "You are an expert security analyst using AgentSmith-HUB, an SDPP (Security Data Pipeline Platform). Your task is to correctly integrate a plugin into a ruleset.\n\n## **Your Task**\nBased on the user's requirements, generate the correct XML snippet for using a plugin.\n- **User's Goal:** %s\n\n---\n\n# üîå AgentSmith-HUB Ruleset Plugin Guide üîå\n\nThere are three distinct ways to use plugins. Choose the right one for your goal.\n\n### **1. For Conditional Checks ‚Üí Use `<node type=\"PLUGIN\">`**\n**Use when:** You need a complex **true/false** decision.\n**Example:** `<node type=\"PLUGIN\">is_internal_ip(_$source_ip)</node>`\n\n### **2. For Data Enrichment ‚Üí Use `<append type=\"PLUGIN\">`**\n**Use when:** You want to **add a new field** with a value calculated by a plugin.\n**Example:** `<append type=\"PLUGIN\" field=\"geo_location_info\">get_geolocation(_$destination_ip)</append>`\n\n### **3. For Performing Actions ‚Üí Use `<plugin>`**\n**Use when:** You want to trigger a **side effect** (e.g., send an alert).\n**Example:** `<plugin>send_alert_to_soc(\"High_Severity_Event\", _$ORIDATA)</plugin>`\n\n#### **Available Plugins**\nHere is a list of plugins currently loaded in the system that you can use:\n\n%s\n\nNow, using this guide, please provide the correct XML snippet for the user's goal."
    },
    {
      "name": "security_audit",
      "description": "Performs a security audit by using tools to gather evidence and then analyzes it.",
      "arguments": [
        {
          "name": "scope",
          "description": "Optional: Area to focus on (e.g., 'authentication', 'data_exposure')",
          "required": false
        }
      ],
      "template": "You are a senior security auditor for AgentSmith-HUB, an SDPP (Security Data Pipeline Platform). Your task is to perform a security audit based on evidence gathered from the live system.\n\n## **Standard Operating Procedure (SOP) for Security Audit**\n\n### **Step 1: Gather Evidence (Required)**\n- **Check for Risky Configurations:** Use `search_components` with queries like 'password', 'token', 'http:' to find potentially insecure configurations.\n- **Review All Components:** Use `get_projects`, `get_inputs`, `get_outputs` to get a list of all active components.\n- **Inspect Specific Components:** For any suspicious components found, use `get_input`, `get_project`, etc. to read their full configuration.\n\n### **Step 2: Analyze and Report**\nBased on the **evidence gathered in Step 1**, provide a security audit report. For each finding, you must:\n- **Cite the Evidence:** Refer to the specific component and configuration that constitutes the risk.\n- **Assess the Risk:** Classify the risk (e.g., Critical, High, Medium, Low).\n- **Provide Actionable Recommendations:** Suggest concrete steps to mitigate the risk.\n\n(This template is a guide; the actual data from tools should drive the final analysis)"
    },
    {
      "name": "cluster_health_check",
      "description": "Performs a cluster health check using system tools and provides a status report.",
      "arguments": [],
      "template": "You are a cluster operations engineer for AgentSmith-HUB, an SDPP (Security Data Pipeline Platform). Your task is to perform a comprehensive health check.\n\n## **Standard Operating Procedure (SOP) for Cluster Health Check**\n\n### **Step 1: Check Cluster-Wide Status (Required)**\n- Use `get_cluster_status` to check node connectivity and leader election.\n- Use `get_cluster_system_metrics` to get an overview of resource usage across all nodes.\n\n### **Step 2: Check Project Health**\n- Use `get_projects` to check the status of all projects. Are there any in an 'error' state?\n- If there are errors, use `get_project_error` to investigate.\n\n### **Step 3: Analyze and Report**\nBased on the **data gathered in the previous steps**, provide a cluster health report including:\n- **Overall Status:** Is the cluster healthy? (Green, Yellow, Red).\n- **Node Health:** Are all nodes online and responsive?\n- **Resource Usage:** Is any node experiencing high CPU or memory usage?\n- **Project Status:** Are all projects running correctly?\n- **Action Items:** List any issues that require attention.\n\n(This template is a guide; the actual data from tools should drive the final analysis)"
    },
    {
      "name": "capacity_planning",
      "description": "Data-driven capacity planning and forecasting.",
      "arguments": [
        {
          "name": "time_horizon",
          "description": "Planning time horizon (e.g., '3_months', '1_year')",
          "required": true
        }
      ],
      "template": "You are a capacity planning specialist for AgentSmith-HUB, an SDPP (Security Data Pipeline Platform). Your task is to create a data-driven capacity forecast.\n\n## **Standard Operating Procedure (SOP) for Capacity Planning**\n\n### **Step 1: Analyze Historical Trends (Required)**\n- Use `get_daily_messages` and `get_qps_stats` to understand data volume and request rate trends over time.\n\n### **Step 2: Assess Current Utilization (Required)**\n- Use `get_cluster_system_stats` to determine the current average and peak resource utilization (CPU/Memory).\n\n### **Step 3: Forecast and Recommend**\nBased on the **data gathered in the previous steps**, create a capacity plan for the specified **Time Horizon: %s**.\n- **Growth Forecast:** Project future data volume and resource needs based on historical trends.\n- **Capacity Recommendations:** Recommend specific scaling actions (e.g., 'Add 2 more nodes', 'Increase memory on all nodes by 50%').\n- **Bottleneck-driven advice**: What is the current bottleneck? CPU, Memory, or IO?\n\n(This template is a guide; the actual data from tools should drive the final analysis)"
    },
    {
      "name": "guide_component_update_workflow",
      "description": "Explains the safe, three-step workflow for updating any component configuration.",
      "arguments": [],
      "template": "You are a senior DevOps engineer for AgentSmith-HUB, an SDPP (Security Data Pipeline Platform). You must explain the standard operating procedure for updating any component.\n\n# Workflow for Safely Updating a Component**\n\nTo ensure system stability, AgentSmith-HUB uses a mandatory three-step workflow for all configuration changes. Direct updates are not permitted. You **MUST** follow these steps.\n\n### **Step 1: Save the Change (Creates a `.new` file)**\nFirst, save your intended changes using the appropriate `update_*` tool. This does **not** apply the change immediately. Instead, it creates a temporary `.new` file, placing the change in a pending state.\n\n- **For projects:** `update_project`\n- **For inputs:** `update_input`\n- **For outputs:** `update_output`\n- **For rulesets:** `update_ruleset`\n- **For plugins:** `update_plugin`\n\n*After this step, you can use `get_pending_changes` to see your change waiting for application.* `\n\n### **Step 2: Apply the Change (Activates the configuration)**\nNext, use the `apply_single_change` tool to make the pending configuration official. This tool replaces the current configuration with the content from the `.new` file.\n\n- **Tool:** `apply_single_change`\n- **Arguments:** `type` (e.g., 'project'), `id` (e.g., 'my_project')\n\n*At this point, the configuration is updated, but the running component is still using the old version.*`\n\n### **Step 3: Restart the Service (Makes the change effective)**\nFinally, to make the running component load the new configuration, you must restart the affected project(s).\n\n- **Tool:** `restart_project`\n- **Argument:** `project_id`\n\n*If you are unsure which projects are affected by your change, use the `get_component_usage` tool first.*\n\nThis three-step process ensures that changes are validated and applied in a controlled manner, minimizing the risk of operational disruption."
    },
    {
      "name": "create_plugin_guide",
      "description": "Comprehensive guide for creating Yaegi plugins with detailed function signature validation based on plugin.Verify() and validatePluginCode()",
      "arguments": [
        {
          "name": "plugin_id",
          "description": "Unique identifier for the plugin",
          "required": true
        },
        {
          "name": "plugin_purpose", 
          "description": "Purpose: 'data_validation', 'field_extraction', 'condition_check', 'data_transformation', or 'custom'",
          "required": false
        },
        {
          "name": "return_type",
          "description": "Plugin return type: 'bool' (for checklist nodes), 'interface{}' (for data processing)",
          "required": false
        }
      ],
      "template": "You are an expert Go developer for AgentSmith-HUB SDPP. Your task is to create a valid Yaegi plugin.\n\n## **Plugin Requirements Analysis**\n- **Plugin ID:** %s\n- **Plugin Purpose:** %s\n- **Return Type:** %s\n\n---\n\n# üîå Plugin Creation Guide (Based on Actual validatePluginCode() Logic)\n\n## **CRITICAL: Understanding Plugin.Verify() Requirements**\nBased on detailed source code analysis, these are the EXACT validation rules:\n\n### **Required Package Structure:**\n```go\npackage plugin\n\n// Required imports - only standard library allowed\nimport (\n    \"errors\"\n    \"strings\"\n    // ... other standard library packages\n)\n\n// REQUIRED: Must have Eval function\nfunc Eval(/* parameters */) (/* return values */) {\n    // Implementation\n}\n```\n\n### **Function Signature Requirements (validateFunctionSignature()):**\n\n**For Checklist Nodes (bool return):**\n```go\nfunc Eval(data string) (bool, error)\nfunc Eval(field1 string, field2 int) (bool, error)\n```\n- **Returns:** `(bool, error)` - exactly 2 values\n- **Usage:** In `<node type=\"PLUGIN\">plugin_name(_$field)</node>`\n\n**For Data Processing (interface{} return):**\n```go\nfunc Eval(data string) (interface{}, bool, error)\nfunc Eval(ip string, port int) (interface{}, bool, error)\n```\n- **Returns:** `(interface{}, bool, error)` - exactly 3 values\n- **Usage:** In `<append type=\"PLUGIN\">plugin_name(_$field)</append>`\n\n### **Parameter Conventions:**\n- **`_$field_name`** - Extract value from message data\n- **`_$ORIDATA`** - Pass entire original message\n- **`\"literal_string\"`** - Pass literal string value\n- **`123`** - Pass literal numeric value\n\n### **Standard Library Restrictions:**\nOnly these imports are allowed (enforced by validatePluginCode()):\n- `errors`, `fmt`, `strings`, `strconv`, `time`\n- `regexp`, `encoding/json`, `net`, `net/url`\n- `crypto/*`, `math`, `sort`, `bytes`\n\n### **Syntax Validation:**\n- Must be valid Go syntax (parsed by go/parser)\n- No external dependencies beyond standard library\n- Must have exactly one `Eval` function\n- Package name must be `plugin`\n\n## **Plugin Examples:**\n\n### **Simple Validation Plugin (bool return):**\n```go\npackage plugin\n\nimport (\n    \"strings\"\n    \"errors\"\n)\n\nfunc Eval(ip string) (bool, error) {\n    if ip == \"\" {\n        return false, errors.New(\"IP cannot be empty\")\n    }\n    \n    // Check if IP is internal\n    return strings.HasPrefix(ip, \"192.168.\") || \n           strings.HasPrefix(ip, \"10.\") ||\n           strings.HasPrefix(ip, \"172.\"), nil\n}\n```\n\n### **Data Enrichment Plugin (interface{} return):**\n```go\npackage plugin\n\nimport (\n    \"encoding/json\"\n    \"strings\"\n)\n\nfunc Eval(data string) (interface{}, bool, error) {\n    if data == \"\" {\n        return nil, false, nil\n    }\n    \n    result := map[string]interface{}{\n        \"processed\": true,\n        \"length\": len(data),\n        \"uppercase\": strings.ToUpper(data),\n    }\n    \n    return result, true, nil\n}\n```\n\n## **Validation Process:**\n1. **Create Plugin**: Use `create_plugin` with proper Go code\n2. **Validate**: Use `verify_component` - checks syntax and signature\n3. **Test**: Use `test_plugin` with sample data\n4. **Apply**: Follow 3-step workflow (update ‚Üí apply ‚Üí restart)\n\n## **Common Validation Errors to Avoid:**\n- Missing package declaration\n- Wrong function signature (incorrect return count/types)\n- External imports (non-standard library)\n- Missing Eval function\n- Invalid Go syntax\n- Wrong package name (must be 'plugin')\n\nNow generate the complete and valid Go plugin code for the specified requirements."
    },
    {
      "name": "component_workflow_guide",
      "description": "Complete guidance for component lifecycle management based on pending_changes.go and components.go analysis",
      "arguments": [
        {
          "name": "component_type",
          "description": "Type of component: 'input', 'output', 'ruleset', 'project', or 'plugin'",
          "required": true
        },
        {
          "name": "workflow_stage",
          "description": "Current workflow stage: 'create', 'update', 'verify', 'commit', 'rollback', or 'restart'",
          "required": true
        },
        {
          "name": "component_id",
          "description": "ID of the component being managed",
          "required": false
        }
      ],
      "template": "You are a senior DevOps engineer for AgentSmith-HUB SDPP. Your task is to guide through component lifecycle management.\n\n## **Workflow Analysis**\n- **Component Type:** %s\n- **Workflow Stage:** %s\n- **Component ID:** %s\n\n---\n\n# üîÑ Component Lifecycle Management (Based on Actual Workflow Analysis)\n\n## **Understanding the .new File System**\nBased on pending_changes.go analysis, AgentSmith-HUB uses a sophisticated change management system:\n\n### **File Extensions:**\n- **`.yaml`** - Formal/active configuration\n- **`.yaml.new`** - Pending changes (inputs, outputs, projects)\n- **`.xml`** - Formal/active ruleset\n- **`.xml.new`** - Pending ruleset changes\n- **`.go`** - Formal/active plugin\n- **`.go.new`** - Pending plugin changes\n\n### **Change States (ChangeStatus):**\n1. **Draft** - Initial change, not verified\n2. **Verified** - Passed validation\n3. **Invalid** - Failed validation\n4. **Applied** - Successfully applied\n5. **Failed** - Application failed\n\n## **Complete Workflow Guide:**\n\n### **Stage 1: Creation/Update**\n```bash\n# Create new component\nuse create_[type] tool with proper configuration\n\n# Update existing component  \nuse update_[type] tool with modified configuration\n```\n**Result:** Creates `.new` file, adds to pending changes\n\n### **Stage 2: Verification**\n```bash\n# Verify single component\nuse verify_component tool\n\n# Verify all pending changes\nuse verify_pending_changes tool\n```\n**Result:** Changes status to 'verified' or 'invalid'\n\n### **Stage 3: Transaction Management**\n```bash\n# Apply single change\nuse apply_single_change tool\n\n# Apply all pending changes (transaction)\nuse apply_pending_changes tool\n\n# Cancel specific change\nuse cancel_pending_change tool\n```\n**Result:** Moves .new ‚Üí formal file, updates global state\n\n### **Stage 4: Service Management**\n```bash\n# Check affected projects\nuse get_component_usage tool\n\n# Restart affected projects\nuse restart_project tool\n\n# Monitor status\nuse get_project tool to check status\n```\n**Result:** Running services use new configuration\n\n## **Enhanced Change Management Features:**\n\n### **Dependency Tracking:**\n- System tracks which projects use each component\n- `get_component_usage` shows impact analysis\n- Automatic project restart recommendations\n\n### **Rollback Procedures:**\n```bash\n# Cancel pending changes before apply\nuse cancel_pending_change\n\n# Revert to previous configuration (manual)\n1. Save current config as backup\n2. Update with previous content\n3. Apply changes\n4. Restart projects\n```\n\n### **Batch Operations:**\n```bash\n# Transaction-based batch apply\nuse apply_pending_changes  # All-or-nothing\n\n# Verify all before apply\nuse verify_pending_changes\n```\n\n### **Error Handling:**\n- Line-number-specific validation errors\n- Detailed error messages with context\n- Automatic rollback on transaction failure\n\n## **Best Practices:**\n\n1. **Always verify before applying**\n2. **Check component usage before changes**\n3. **Use batch operations for related changes**\n4. **Monitor project status after restarts**\n5. **Backup configurations before major changes**\n\n## **Troubleshooting Common Issues:**\n\n- **\"Component not found\"** ‚Üí Check if formal component exists (not just .new)\n- **\"Project startup failed\"** ‚Üí Check component dependencies\n- **\"Validation failed\"** ‚Üí Review line-specific errors from verify_component\n- **\"Apply failed\"** ‚Üí Check file permissions and disk space\n\nNow proceeding with the specific workflow stage guidance for your current situation."
    },
    {
      "name": "test_components_guide",
      "description": "Comprehensive guide for testing all component types with detailed testing strategies based on actual testing.go implementation and test capabilities",
      "arguments": [
        {
          "name": "component_type",
          "description": "Type of component to test: 'input', 'output', 'ruleset', 'plugin', 'project', or 'all'",
          "required": true
        },
        {
          "name": "component_id",
          "description": "ID of the specific component to test",
          "required": false
        },
        {
          "name": "test_type",
          "description": "Type of test: 'connectivity', 'functionality', 'integration', 'performance', or 'comprehensive'",
          "required": false
        },
        {
          "name": "test_data_type",
          "description": "Type of test data: 'sample', 'realistic', 'edge_cases', 'stress', or 'custom'",
          "required": false
        }
      ],
      "template": "You are an expert QA engineer for AgentSmith-HUB SDPP. Your task is to provide comprehensive component testing guidance.\n\n## **Testing Request Analysis**\n- **Component Type:** %s\n- **Component ID:** %s\n- **Test Type:** %s\n- **Test Data Type:** %s\n\n---\n\n# üß™ Component Testing Guide (Based on Actual testing.go Implementation)\n\n## **CRITICAL: Understanding AgentSmith-HUB Testing Architecture**\nBased on detailed analysis of testing.go, the system provides comprehensive testing capabilities:\n\n### **Available Testing Tools:**\n- `test_plugin` / `test_plugin_content` - Plugin logic testing\n- `test_ruleset` / `test_ruleset_content` - Ruleset rule testing\n- `test_output` - Output delivery testing\n- `test_project` / `test_project_content` - End-to-end pipeline testing\n- `connect_check` - Connectivity testing for inputs/outputs\n- `verify_component` - Configuration validation\n\n## **Component-Specific Testing Strategies:**\n\n### **1. Input Component Testing**\n\n**Connectivity Testing:**\n```bash\n# Test connection to external systems\nuse connect_check with type='input' and id='component_id'\n\n# Test with pending changes\nuse connect_check after update_input (tests .new file)\n```\n\n**Validation Strategy:**\n- ‚úÖ Broker/endpoint reachability\n- ‚úÖ Authentication credentials\n- ‚úÖ Topic/logstore existence\n- ‚úÖ Permission validation\n- ‚úÖ Compression settings\n- ‚úÖ Consumer group conflicts\n\n**Test Data Scenarios:**\n- **Network failures**: Unreachable brokers\n- **Authentication failures**: Invalid credentials\n- **Permission issues**: Unauthorized access\n- **Configuration errors**: Invalid parameters\n\n### **2. Output Component Testing**\n\n**Delivery Testing:**\n```bash\n# Test message delivery capability\nuse test_output with id='output_id' and test_data='{\"sample\": \"message\"}'\n\n# Test connectivity\nuse connect_check with type='output' and id='output_id'\n```\n\n**Validation Strategy:**\n- ‚úÖ Destination connectivity\n- ‚úÖ Message formatting\n- ‚úÖ Batch processing\n- ‚úÖ Delivery confirmation\n- ‚úÖ Error handling\n- ‚úÖ Performance metrics\n\n**Test Data Scenarios:**\n- **Single message**: Basic delivery test\n- **Batch messages**: Volume handling\n- **Large payloads**: Size limits\n- **Special characters**: Encoding issues\n- **Malformed data**: Error handling\n\n### **3. Plugin Component Testing**\n\n**Function Logic Testing:**\n```bash\n# Test existing plugin\nuse test_plugin with id='plugin_id' and test_data='{\"field\": \"value\"}'\n\n# Test code without saving\nuse test_plugin_content with content='plugin_code' and test_data='{\"test\": \"data\"}'\n```\n\n**Validation Strategy:**\n- ‚úÖ Function signature compliance\n- ‚úÖ Return value correctness\n- ‚úÖ Error handling\n- ‚úÖ Parameter validation\n- ‚úÖ Edge case behavior\n- ‚úÖ Performance benchmarks\n\n**Test Data Scenarios:**\n- **Valid inputs**: Expected use cases\n- **Invalid inputs**: Error handling\n- **Edge cases**: Boundary conditions\n- **Empty/null values**: Null safety\n- **Large datasets**: Performance testing\n- **Unicode/special chars**: Encoding support\n\n### **4. Ruleset Component Testing**\n\n**Rule Logic Testing:**\n```bash\n# Test existing ruleset\nuse test_ruleset with id='ruleset_id' and test_data='{\"event\": \"data\"}'\n\n# Test XML without saving\nuse test_ruleset_content with content='xml_content' and test_data='{\"test\": \"event\"}'\n```\n\n**Validation Strategy:**\n- ‚úÖ Rule matching accuracy\n- ‚úÖ Filter efficiency\n- ‚úÖ Condition logic\n- ‚úÖ Plugin integration\n- ‚úÖ Threshold detection\n- ‚úÖ Data transformation\n- ‚úÖ Performance impact\n\n**Test Data Scenarios:**\n- **Positive matches**: Rules should trigger\n- **Negative matches**: Rules should not trigger\n- **Edge conditions**: Boundary testing\n- **Complex events**: Multi-field logic\n- **Threshold events**: Statistical detection\n- **Plugin data**: External function calls\n\n### **5. Project Component Testing**\n\n**End-to-End Pipeline Testing:**\n```bash\n# Test complete data flow\nuse test_project with id='project_id', input_node='input.source', test_data='{\"pipeline\": \"test\"}'\n\n# Test configuration without saving\nuse test_project_content with content='project_yaml', input_node='input.node', test_data='{\"flow\": \"test\"}'\n```\n\n**Validation Strategy:**\n- ‚úÖ Data flow correctness\n- ‚úÖ Component integration\n- ‚úÖ Message transformation\n- ‚úÖ Output generation\n- ‚úÖ Error propagation\n- ‚úÖ Performance metrics\n- ‚úÖ Resource utilization\n\n**Test Data Scenarios:**\n- **Simple flow**: INPUT ‚Üí RULESET ‚Üí OUTPUT\n- **Branched flow**: One input, multiple outputs\n- **Merged flow**: Multiple inputs, one output\n- **Complex flow**: Branching and merging\n- **Error injection**: Component failures\n- **Load testing**: High-volume data\n\n## **Advanced Testing Strategies:**\n\n### **Comprehensive Testing Workflow:**\n\n**1. Pre-Testing Validation:**\n```bash\n# Always verify configuration first\nuse verify_component with type and id\n\n# Check component dependencies\nuse get_component_usage with component_id\n\n# Review pending changes\nuse get_pending_changes\n```\n\n**2. Isolated Component Testing:**\n```bash\n# Test individual components in isolation\nuse test_plugin for business logic\nuse test_ruleset for detection rules\nuse test_output for delivery\nuse connect_check for connectivity\n```\n\n**3. Integration Testing:**\n```bash\n# Test component interactions\nuse test_project for end-to-end flows\nuse get_project_inputs for flow validation\nuse get_project_components for dependency check\n```\n\n**4. Performance Testing:**\n```bash\n# Monitor system impact\nuse get_system_stats before and after tests\nuse get_qps_stats for throughput analysis\nuse get_cluster_system_metrics for resource usage\n```\n\n### **Test Data Design Patterns:**\n\n**Security Event Testing:**\n```json\n{\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"source_ip\": \"192.168.1.100\",\n  \"process_name\": \"powershell.exe\",\n  \"command_line\": \"powershell -enc <base64>\",\n  \"user\": \"administrator\",\n  \"event_type\": \"process_creation\"\n}\n```\n\n**Network Traffic Testing:**\n```json\n{\n  \"src_ip\": \"10.0.1.50\",\n  \"dst_ip\": \"8.8.8.8\",\n  \"src_port\": 12345,\n  \"dst_port\": 53,\n  \"protocol\": \"UDP\",\n  \"bytes\": 64,\n  \"packets\": 1\n}\n```\n\n**File Integrity Testing:**\n```json\n{\n  \"file_path\": \"/etc/passwd\",\n  \"action\": \"modified\",\n  \"user\": \"root\",\n  \"hash_before\": \"abc123\",\n  \"hash_after\": \"def456\",\n  \"timestamp\": \"2024-01-01T12:00:00Z\"\n}\n```\n\n### **Error Testing Scenarios:**\n\n**Configuration Errors:**\n- Invalid YAML/XML syntax\n- Missing required fields\n- Invalid component references\n- Circular dependencies\n\n**Runtime Errors:**\n- Network connectivity failures\n- Authentication timeouts\n- Resource exhaustion\n- External service unavailability\n\n**Logic Errors:**\n- Incorrect rule conditions\n- Plugin execution failures\n- Data transformation errors\n- Filter inefficiencies\n\n### **Testing Best Practices:**\n\n**1. Test Environment Isolation:**\n- Testing uses temporary component instances\n- No impact on production configurations\n- Automatic cleanup after testing\n- Independent test channels and data flows\n\n**2. Comprehensive Coverage:**\n- Test all code paths and conditions\n- Include positive and negative scenarios\n- Validate edge cases and error conditions\n- Performance test with realistic data volumes\n\n**3. Data-Driven Testing:**\n- Use realistic sample data\n- Include various data formats and encodings\n- Test with different message sizes\n- Validate special character handling\n\n**4. Continuous Testing:**\n- Test after every configuration change\n- Validate before applying changes\n- Monitor post-deployment behavior\n- Regular connectivity health checks\n\n## **Testing Troubleshooting:**\n\n**Common Test Failures:**\n- **\"Component not found\"** ‚Üí Verify component exists and is saved\n- **\"Connection failed\"** ‚Üí Check network connectivity and credentials\n- **\"Plugin execution failed\"** ‚Üí Review plugin syntax and logic\n- **\"Ruleset parsing error\"** ‚Üí Validate XML syntax and structure\n- **\"Project flow error\"** ‚Üí Check component dependencies and data flow syntax\n\n**Performance Issues:**\n- **Slow response** ‚Üí Check system resources and data volume\n- **Timeout errors** ‚Üí Increase timeout or optimize component logic\n- **Memory issues** ‚Üí Review batch sizes and data processing efficiency\n\nNow provide specific testing guidance based on your component and requirements."
    },
    {
      "name": "troubleshoot_connectivity",
      "description": "Comprehensive troubleshooting guide for connectivity issues with input/output components and external systems",
      "arguments": [
        {
          "name": "component_type",
          "description": "Type of component with connectivity issues: 'input', 'output', or 'all'",
          "required": true
        },
        {
          "name": "component_id",
          "description": "ID of the specific component with issues",
          "required": false
        },
        {
          "name": "error_symptoms",
          "description": "Description of observed connectivity symptoms or error messages",
          "required": false
        },
        {
          "name": "troubleshooting_level",
          "description": "Level of troubleshooting: 'basic', 'advanced', or 'expert'",
          "required": false
        }
      ],
      "template": "You are a senior network and systems engineer for AgentSmith-HUB SDPP. Your task is to diagnose and resolve connectivity issues.\n\n## **Troubleshooting Request**\n- **Component Type:** %s\n- **Component ID:** %s\n- **Error Symptoms:** %s\n- **Troubleshooting Level:** %s\n\n---\n\n# üîß Connectivity Troubleshooting Guide\n\n## **CRITICAL: Understanding AgentSmith-HUB Connectivity Architecture**\nBased on API analysis and connect_check implementation:\n\n### **Available Diagnostic Tools:**\n- `connect_check` - Test connectivity for input/output components\n- `verify_component` - Validate component configuration\n- `get_error_logs` - Check local error logs\n- `get_cluster_error_logs` - Check cluster-wide errors\n- `get_system_metrics` - Monitor system resources\n- `test_[component]` - Test component functionality\n\n## **Systematic Troubleshooting Workflow:**\n\n### **Phase 1: Initial Assessment**\n```bash\n# Check component status\nuse get_[inputs|outputs] to list all components\n\n# Verify configuration\nuse verify_component with type and id\n\n# Test connectivity\nuse connect_check with type and id\n```\n\n### **Phase 2: Error Analysis**\n```bash\n# Check error logs\nuse get_error_logs for local issues\nuse get_cluster_error_logs for cluster-wide issues\n\n# Monitor system resources\nuse get_system_metrics\nuse get_cluster_system_metrics\n```\n\n### **Phase 3: Component-Specific Diagnostics**\n\n**For Input Components:**\n- ‚úÖ Broker/endpoint accessibility\n- ‚úÖ Authentication credentials validity\n- ‚úÖ Network path and DNS resolution\n- ‚úÖ Port availability and firewall rules\n- ‚úÖ Topic/logstore existence and permissions\n- ‚úÖ Consumer group conflicts\n- ‚úÖ Protocol version compatibility\n\n**For Output Components:**\n- ‚úÖ Destination reachability\n- ‚úÖ Authentication and authorization\n- ‚úÖ Index/topic creation permissions\n- ‚úÖ Message format compatibility\n- ‚úÖ Batch size and timeout settings\n- ‚úÖ Retry policy configuration\n\n## **Common Connectivity Issues & Solutions:**\n\n### **Authentication Failures**\n**Symptoms:** 401/403 errors, \"Invalid credentials\"\n**Diagnosis:**\n```bash\nuse connect_check with type='input/output' and id='component_id'\n```\n**Solutions:**\n- Verify credential validity and expiration\n- Check authentication method (token, key, certificate)\n- Validate permission scopes and roles\n- Test credentials outside AgentSmith-HUB\n\n### **Network Connectivity Issues**\n**Symptoms:** Timeouts, \"Connection refused\", DNS errors\n**Diagnosis:**\n```bash\n# Check system connectivity\nuse get_system_metrics\nuse get_cluster_system_metrics\n\n# Review error patterns\nuse get_error_logs\n```\n**Solutions:**\n- Verify hostname/IP reachability\n- Check firewall rules and security groups\n- Validate DNS resolution\n- Test port accessibility\n- Check proxy/gateway configuration\n\n### **Configuration Errors**\n**Symptoms:** Validation failures, malformed requests\n**Diagnosis:**\n```bash\nuse verify_component with type and id\nuse get_[component] with id to review config\n```\n**Solutions:**\n- Validate YAML/JSON syntax\n- Check required field completeness\n- Verify parameter value formats\n- Review component-specific requirements\n\n### **Resource Exhaustion**\n**Symptoms:** Slow responses, timeouts, memory errors\n**Diagnosis:**\n```bash\nuse get_system_metrics\nuse get_cluster_system_metrics\nuse get_qps_stats for throughput analysis\n```\n**Solutions:**\n- Monitor CPU, memory, disk usage\n- Adjust batch sizes and timeouts\n- Scale cluster resources\n- Optimize component configurations\n\n### **Service Availability Issues**\n**Symptoms:** Intermittent failures, service errors\n**Diagnosis:**\n```bash\nuse connect_check repeatedly over time\nuse get_cluster_status for cluster health\n```\n**Solutions:**\n- Check external service status\n- Implement retry policies\n- Configure health checks\n- Set up monitoring alerts\n\n## **Advanced Troubleshooting Techniques:**\n\n### **Network Layer Debugging**\n- Use network tools (ping, telnet, curl) for layer-by-layer testing\n- Check intermediate network devices (proxies, load balancers)\n- Validate SSL/TLS certificate chains\n- Monitor network latency and packet loss\n\n### **Application Layer Analysis**\n- Analyze message formats and encoding\n- Test with minimal configurations\n- Compare working vs. failing components\n- Use protocol-specific debugging tools\n\n### **Cluster-Wide Issues**\n```bash\n# Check cluster synchronization\nuse get_cluster_status\nuse get_cluster_error_logs\n\n# Monitor cross-node communication\nuse get_cluster_system_metrics\n```\n\n## **Prevention and Monitoring:**\n\n### **Proactive Monitoring Setup**\n```bash\n# Regular health checks\nuse connect_check scheduled automatically\n\n# Resource monitoring\nuse get_system_metrics with alerting\n\n# Error pattern detection\nuse get_error_logs with analysis\n```\n\n### **Configuration Best Practices**\n- Use connection pooling for efficiency\n- Implement proper timeout values\n- Configure retry policies with exponential backoff\n- Set up health check endpoints\n- Document connectivity requirements\n\n### **Maintenance Procedures**\n- Regular connectivity testing\n- Periodic credential rotation\n- Network path validation\n- Performance baseline establishment\n- Disaster recovery testing\n\nNow provide specific troubleshooting guidance based on your connectivity issue."
    },
    {
      "name": "analyze_performance_issues",
      "description": "Comprehensive performance analysis and optimization guide for AgentSmith-HUB components and system performance",
      "arguments": [
        {
          "name": "performance_scope",
          "description": "Scope of performance analysis: 'component', 'project', 'cluster', or 'system'",
          "required": true
        },
        {
          "name": "component_id",
          "description": "Specific component ID if analyzing component performance",
          "required": false
        },
        {
          "name": "performance_metric",
          "description": "Primary performance concern: 'throughput', 'latency', 'memory', 'cpu', or 'all'",
          "required": false
        },
        {
          "name": "time_window",
          "description": "Time window for analysis: 'current', 'hourly', 'daily', or 'trending'",
          "required": false
        }
      ],
      "template": "You are a performance engineering expert for AgentSmith-HUB SDPP. Your task is to analyze and optimize system performance.\n\n## **Performance Analysis Request**\n- **Analysis Scope:** %s\n- **Component ID:** %s\n- **Performance Metric:** %s\n- **Time Window:** %s\n\n---\n\n# üìä Performance Analysis & Optimization Guide\n\n## **CRITICAL: AgentSmith-HUB Performance Monitoring Tools**\nBased on comprehensive API analysis:\n\n### **Performance Metrics APIs:**\n- `get_qps_data` - Real-time QPS metrics\n- `get_qps_stats` - Statistical QPS analysis\n- `get_hourly_messages` - Hourly throughput patterns\n- `get_daily_messages` - Daily throughput trends\n- `get_system_metrics` - System resource utilization\n- `get_system_stats` - Historical system performance\n- `get_cluster_system_metrics` - Cluster-wide performance\n- `get_cluster_system_stats` - Cluster statistical analysis\n\n## **Systematic Performance Analysis Workflow:**\n\n### **Phase 1: Baseline Assessment**\n```bash\n# Get current performance snapshot\nuse get_qps_data for real-time metrics\nuse get_system_metrics for resource usage\nuse get_cluster_system_metrics for cluster view\n\n# Analyze historical trends\nuse get_qps_stats for throughput analysis\nuse get_hourly_messages for pattern detection\nuse get_daily_messages for long-term trends\n```\n\n### **Phase 2: Bottleneck Identification**\n```bash\n# Component-level analysis\nuse get_project_components with id for component usage\nuse test_[component] with performance test data\n\n# System-level analysis\nuse get_system_stats for resource trends\nuse get_error_logs for performance-related errors\n```\n\n### **Phase 3: Root Cause Analysis**\n```bash\n# Deep dive into specific issues\nuse get_cluster_error_logs for distributed issues\nuse get_component_usage for dependency analysis\nuse verify_component for configuration issues\n```\n\n## **Performance Analysis by Scope:**\n\n### **Component-Level Performance**\n\n**Input Component Analysis:**\n- **Ingestion Rate**: Messages per second capacity\n- **Connection Pool**: Concurrent connection efficiency\n- **Buffer Management**: Queue depth and overflow handling\n- **Protocol Overhead**: Serialization/deserialization costs\n- **Authentication Latency**: Auth token refresh frequency\n\n**Ruleset Component Analysis:**\n- **Rule Evaluation Time**: Processing latency per rule\n- **Memory Usage**: Rule engine memory consumption\n- **Plugin Execution**: Custom plugin performance impact\n- **Field Processing**: Data transformation overhead\n- **Condition Complexity**: Logical operation efficiency\n\n**Output Component Analysis:**\n- **Delivery Throughput**: Messages delivered per second\n- **Batch Efficiency**: Optimal batch size configuration\n- **Connection Reuse**: Output connection pooling\n- **Retry Overhead**: Failed delivery retry impact\n- **Serialization Cost**: Output format conversion time\n\n### **Project-Level Performance**\n\n**Data Flow Analysis:**\n```bash\n# Analyze project pipeline performance\nuse test_project with performance test data\nuse get_project_component_sequences for flow analysis\n```\n\n**Pipeline Bottlenecks:**\n- **Input Saturation**: Input component capacity limits\n- **Processing Delays**: Ruleset evaluation bottlenecks\n- **Output Queuing**: Output component backpressure\n- **Memory Pressure**: Project memory consumption\n- **Inter-Component Latency**: Message passing overhead\n\n### **System-Level Performance**\n\n**Resource Utilization:**\n- **CPU Usage**: Processing load distribution\n- **Memory Consumption**: Memory allocation patterns\n- **Disk I/O**: File system and logging overhead\n- **Network Bandwidth**: Communication efficiency\n- **Goroutine Count**: Concurrency management\n\n**Cluster Performance:**\n- **Node Load Balancing**: Work distribution across nodes\n- **Inter-Node Communication**: Cluster synchronization overhead\n- **Leader Election Impact**: Leadership change performance\n- **Configuration Sync**: Configuration distribution efficiency\n\n## **Performance Optimization Strategies:**\n\n### **Throughput Optimization**\n\n**Input Optimization:**\n```yaml\n# High-throughput input configuration\nkafka:\n  batch_size: 1000\n  max_wait_time: 100ms\n  buffer_size: 10000\n  compression: snappy\n  connection_pool_size: 10\n```\n\n**Output Optimization:**\n```yaml\n# High-throughput output configuration\nelasticsearch:\n  batch_size: 500\n  flush_duration: 1s\n  max_retries: 3\n  connection_pool_size: 5\n  compression: gzip\n```\n\n### **Latency Optimization**\n\n**Real-time Processing:**\n```yaml\n# Low-latency configuration\ninput:\n  batch_size: 1\n  max_wait_time: 10ms\noutput:\n  batch_size: 1\n  flush_duration: 100ms\n```\n\n**Ruleset Optimization:**\n- Simplify rule conditions\n- Optimize plugin implementations\n- Use efficient field operations\n- Minimize regex complexity\n- Cache expensive computations\n\n### **Memory Optimization**\n\n**Configuration Tuning:**\n```yaml\n# Memory-efficient settings\nsystem:\n  gc_target_percentage: 80\n  max_goroutines: 1000\nbuffering:\n  max_buffer_size: 1000\n  memory_limit: 100MB\n```\n\n**Best Practices:**\n- Use streaming processing instead of batching\n- Implement proper garbage collection\n- Monitor memory leak patterns\n- Optimize data structures\n- Use object pooling for frequent allocations\n\n### **CPU Optimization**\n\n**Concurrency Tuning:**\n- Balance goroutine count with CPU cores\n- Optimize context switching overhead\n- Use CPU-efficient algorithms\n- Implement proper load balancing\n- Monitor CPU utilization patterns\n\n## **Performance Monitoring Best Practices:**\n\n### **Continuous Monitoring Setup**\n```bash\n# Automated performance monitoring\nuse get_qps_stats scheduled every 5 minutes\nuse get_system_metrics scheduled every minute\nuse get_hourly_messages for trend analysis\n```\n\n### **Alert Configuration**\n- QPS drops below threshold\n- Memory usage exceeds 80%\n- CPU usage sustained above 90%\n- Error rate increases significantly\n- Response time exceeds SLA\n\n### **Performance Baselines**\n- Establish normal operating parameters\n- Document expected performance ranges\n- Track performance regression over time\n- Set up automated performance testing\n- Create performance dashboards\n\n## **Troubleshooting Performance Issues:**\n\n### **High CPU Usage**\n**Diagnosis:**\n```bash\nuse get_system_metrics\nuse get_cluster_system_metrics for cluster view\n```\n**Solutions:**\n- Optimize rule complexity\n- Reduce plugin computational overhead\n- Balance load across cluster nodes\n- Implement CPU-efficient algorithms\n\n### **High Memory Usage**\n**Diagnosis:**\n```bash\nuse get_system_stats for memory trends\nuse get_error_logs for memory-related errors\n```\n**Solutions:**\n- Reduce buffer sizes\n- Implement memory pooling\n- Fix memory leaks\n- Optimize data structures\n\n### **Low Throughput**\n**Diagnosis:**\n```bash\nuse get_qps_data for current throughput\nuse get_qps_stats for historical comparison\n```\n**Solutions:**\n- Increase batch sizes\n- Optimize connection pooling\n- Reduce processing complexity\n- Scale cluster resources\n\nNow provide specific performance analysis and optimization recommendations."
    },
    {
      "name": "manage_bulk_operations",
      "description": "Comprehensive guide for managing bulk operations including batch updates, mass deployments, and coordinated changes across multiple components",
      "arguments": [
        {
          "name": "operation_type",
          "description": "Type of bulk operation: 'update', 'deploy', 'restart', 'verify', or 'rollback'",
          "required": true
        },
        {
          "name": "target_scope",
          "description": "Scope of operation: 'projects', 'components', 'cluster', or 'system'",
          "required": true
        },
        {
          "name": "batch_size",
          "description": "Preferred batch size for operation execution",
          "required": false
        },
        {
          "name": "risk_level",
          "description": "Risk tolerance level: 'low', 'medium', 'high'",
          "required": false
        }
      ],
      "template": "You are a senior operations engineer for AgentSmith-HUB SDPP. Your task is to plan and execute bulk operations safely and efficiently.\n\n## **Bulk Operation Request**\n- **Operation Type:** %s\n- **Target Scope:** %s\n- **Batch Size:** %s\n- **Risk Level:** %s\n\n---\n\n# üîÑ Bulk Operations Management Guide\n\n## **CRITICAL: AgentSmith-HUB Bulk Operation Tools**\nBased on comprehensive API analysis:\n\n### **Bulk Operation APIs:**\n- `apply_changes` - Apply all pending changes\n- `apply_changes_enhanced` - Enhanced batch operations\n- `verify_changes` - Validate all pending changes\n- `restart_all_projects` - Restart all projects\n- `cancel_all_changes` - Cancel all pending changes\n- `get_enhanced_pending_changes` - Detailed change analysis\n- `load_local_changes` - Bulk load from filesystem\n\n### **Analysis and Planning APIs:**\n- `get_component_usage` - Dependency impact analysis\n- `get_cluster_status` - Cluster readiness assessment\n- `get_system_metrics` - Resource capacity planning\n\n## **Bulk Operation Planning Framework:**\n\n### **Phase 1: Pre-Operation Assessment**\n\n**Risk Assessment:**\n```bash\n# Analyze current system state\nuse get_cluster_status\nuse get_system_metrics\nuse get_cluster_system_metrics\n\n# Review pending changes\nuse get_enhanced_pending_changes\nuse verify_changes for validation\n```\n\n**Dependency Analysis:**\n```bash\n# Map component dependencies\nuse get_component_usage for each target component\nuse get_project_components for project analysis\nuse get_project_component_sequences for flow analysis\n```\n\n**Capacity Planning:**\n```bash\n# Assess system capacity\nuse get_qps_stats for current load\nuse get_system_stats for resource trends\nuse get_cluster_system_stats for cluster capacity\n```\n\n### **Phase 2: Operation Sequencing**\n\n**Dependency-Based Ordering:**\n1. **Independent Components First**\n   - Components with no dependencies\n   - Leaf nodes in dependency graph\n   - Non-critical supporting components\n\n2. **Core Components Second**\n   - Central processing components\n   - High-dependency components\n   - Critical path elements\n\n3. **Integration Points Last**\n   - Cross-system integrations\n   - External service connections\n   - Cluster coordination points\n\n### **Phase 3: Batch Execution Strategy**\n\n**Conservative Approach (Low Risk):**\n- Batch size: 1-3 components\n- Validation after each batch\n- Manual approval between batches\n- Immediate rollback on first failure\n\n**Balanced Approach (Medium Risk):**\n- Batch size: 5-10 components\n- Automated validation between batches\n- Pause on critical failures\n- Staged rollback procedures\n\n**Aggressive Approach (High Risk):**\n- Batch size: 10+ components\n- Continuous execution\n- Post-operation validation\n- Bulk rollback procedures\n\n## **Operation-Specific Workflows:**\n\n### **Bulk Component Updates**\n\n**Preparation:**\n```bash\n# Review all pending changes\nuse get_enhanced_pending_changes\n\n# Validate changes\nuse verify_changes\n\n# Assess impact\nfor each component:\n  use get_component_usage with type and id\n```\n\n**Execution:**\n```bash\n# Option 1: All at once (high risk)\nuse apply_changes_enhanced\n\n# Option 2: Controlled batches (recommended)\nuse apply_single_change for each component in sequence\n\n# Monitor progress\nuse get_pending_changes to track remaining\n```\n\n**Validation:**\n```bash\n# Verify successful application\nuse verify_component for each updated component\nuse get_error_logs for issues\nuse test_[component] for functionality verification\n```\n\n### **Mass Project Deployment**\n\n**Pre-Deployment:**\n```bash\n# Check project configurations\nuse get_projects\nfor each project:\n  use verify_component with type='project' and id\n  use get_project_components for dependency check\n```\n\n**Deployment:**\n```bash\n# Apply all pending project changes\nuse apply_changes_enhanced\n\n# Restart projects in dependency order\nuse restart_all_projects\n\n# Alternative: Individual project restart\nfor each project in order:\n  use restart_project with project_id\n```\n\n**Post-Deployment:**\n```bash\n# Verify project status\nuse get_projects\nfor each project:\n  use get_project with id\n  use get_project_error if status=error\n```\n\n### **Cluster-Wide Operations**\n\n**Preparation:**\n```bash\n# Assess cluster health\nuse get_cluster_status\nuse get_cluster_system_metrics\nuse get_cluster_error_logs\n```\n\n**Execution:**\n```bash\n# Synchronize configurations\nuse component_sync with cluster data\nuse project_status_sync with status data\n\n# Apply changes cluster-wide\nuse apply_changes_enhanced\nuse restart_all_projects\n```\n\n**Validation:**\n```bash\n# Verify cluster consistency\nuse get_cluster_status\nuse get_cluster_system_metrics\nuse get_cluster_error_logs\n```\n\n## **Error Handling and Recovery:**\n\n### **Failure Detection**\n```bash\n# Monitor for failures during operation\nuse get_error_logs\nuse get_cluster_error_logs\nuse get_system_metrics for resource issues\n```\n\n### **Rollback Strategies**\n\n**Partial Rollback:**\n```bash\n# Cancel remaining operations\nuse cancel_all_changes\n\n# Revert failed components\nfor each failed component:\n  restore previous configuration\n  use update_[component] with previous content\n  use apply_single_change\n```\n\n**Complete Rollback:**\n```bash\n# Cancel all pending changes\nuse cancel_all_changes\n\n# Restore system to pre-operation state\nuse load_local_changes with backup configurations\nuse restart_all_projects\n```\n\n### **Recovery Procedures**\n\n**Component Recovery:**\n1. Identify failed components\n2. Restore previous configurations\n3. Verify component functionality\n4. Restart dependent projects\n5. Validate data flow restoration\n\n**System Recovery:**\n1. Stop all processing\n2. Restore configuration backup\n3. Restart cluster nodes\n4. Verify cluster synchronization\n5. Resume processing gradually\n\n## **Best Practices for Bulk Operations:**\n\n### **Planning Guidelines**\n- Always create backups before bulk operations\n- Test procedures in staging environment\n- Document rollback procedures\n- Establish communication protocols\n- Set up monitoring and alerting\n\n### **Execution Guidelines**\n- Start with non-critical components\n- Validate each batch before proceeding\n- Monitor system resources continuously\n- Be prepared to halt on first major failure\n- Document all changes and decisions\n\n### **Post-Operation Guidelines**\n- Verify all components are functioning\n- Check data flow integrity\n- Monitor performance for degradation\n- Update documentation and runbooks\n- Conduct post-operation review\n\n## **Monitoring and Alerting**\n\n### **Operation Monitoring**\n```bash\n# Continuous monitoring during operation\nuse get_system_metrics every 30 seconds\nuse get_error_logs every minute\nuse get_qps_data for throughput monitoring\n```\n\n### **Success Metrics**\n- Component update success rate\n- Project restart success rate\n- System performance maintenance\n- Error rate within acceptable limits\n- Data flow continuity\n\n### **Failure Indicators**\n- Component validation failures\n- Project startup failures\n- System resource exhaustion\n- Increased error rates\n- Data flow disruption\n\nNow provide specific bulk operation guidance based on your requirements."
    },
    {
      "name": "assess_change_impact",
      "description": "Comprehensive change impact assessment and risk analysis for component modifications, updates, and deployments",
      "arguments": [
        {
          "name": "change_type",
          "description": "Type of change: 'component_update', 'new_component', 'deletion', 'configuration', or 'infrastructure'",
          "required": true
        },
        {
          "name": "component_type",
          "description": "Component type affected: 'input', 'output', 'ruleset', 'plugin', 'project', or 'multiple'",
          "required": false
        },
        {
          "name": "component_id",
          "description": "Specific component ID for focused analysis",
          "required": false
        },
        {
          "name": "assessment_depth",
          "description": "Depth of impact analysis: 'basic', 'detailed', or 'comprehensive'",
          "required": false
        }
      ],
      "template": "You are a change management specialist for AgentSmith-HUB SDPP. Your task is to assess change impact and provide risk analysis.\n\n## **Change Impact Assessment Request**\n- **Change Type:** %s\n- **Component Type:** %s\n- **Component ID:** %s\n- **Assessment Depth:** %s\n\n---\n\n# üîç Change Impact Assessment & Risk Analysis\n\n## **CRITICAL: AgentSmith-HUB Change Analysis Tools**\nBased on comprehensive API and dependency analysis:\n\n### **Impact Analysis APIs:**\n- `get_component_usage` - Component dependency analysis\n- `get_enhanced_pending_changes` - Detailed change assessment\n- `verify_changes` - Change validation and conflict detection\n- `get_project_components` - Project component relationships\n- `get_project_component_sequences` - Data flow impact analysis\n\n### **System State APIs:**\n- `get_cluster_status` - Cluster impact assessment\n- `get_projects` - Project status evaluation\n- `get_qps_stats` - Performance impact baseline\n- `get_system_metrics` - Resource impact analysis\n\n## **Change Impact Analysis Framework:**\n\n### **Phase 1: Direct Impact Assessment**\n\n**Component Dependency Analysis:**\n```bash\n# Identify direct dependencies\nuse get_component_usage with type and id\n\n# Map component relationships\nuse get_project_components for each affected project\nuse get_project_component_sequences for flow analysis\n```\n\n**Configuration Impact:**\n```bash\n# Validate proposed changes\nuse verify_change with type and id\nuse verify_changes for batch validation\n\n# Check configuration conflicts\nuse get_enhanced_pending_changes\n```\n\n### **Phase 2: Cascade Impact Analysis**\n\n**Downstream Effects:**\n1. **Direct Dependents**\n   - Projects using the component\n   - Components that reference it\n   - Data flows that include it\n\n2. **Indirect Dependents**\n   - Projects depending on direct dependents\n   - Cross-component integrations\n   - Cluster-wide configurations\n\n3. **System-wide Effects**\n   - Performance implications\n   - Resource utilization changes\n   - Capacity requirements\n\n### **Phase 3: Risk Assessment Matrix**\n\n**Risk Factors:**\n- **Criticality**: Component importance to business operations\n- **Complexity**: Technical complexity of the change\n- **Dependencies**: Number and importance of dependent components\n- **Reversibility**: Ease of rollback if issues occur\n- **Testing**: Availability of comprehensive testing\n\n## **Change Type Specific Analysis:**\n\n### **Component Update Impact**\n\n**Configuration Changes:**\n```bash\n# Analyze current component usage\nuse get_component_usage with type and id\n\n# Identify affected projects\nfor each dependent project:\n  use get_project with id\n  use get_project_component_sequences with id\n```\n\n**Functional Changes:**\n- **Input Components**: Message processing changes, format modifications\n- **Output Components**: Delivery method changes, format alterations\n- **Rulesets**: Rule logic changes, field processing modifications\n- **Plugins**: Function signature changes, logic modifications\n- **Projects**: Data flow changes, component relationship updates\n\n**Risk Assessment:**\n- **Low Risk**: Parameter tuning, performance optimization\n- **Medium Risk**: New features, backward-compatible changes\n- **High Risk**: Breaking changes, major functionality modifications\n\n### **New Component Impact**\n\n**System Integration:**\n```bash\n# Assess system capacity\nuse get_system_metrics\nuse get_cluster_system_metrics\n\n# Evaluate performance impact\nuse get_qps_stats for baseline\n```\n\n**Resource Requirements:**\n- **CPU Impact**: Additional processing load\n- **Memory Impact**: New component memory requirements\n- **Network Impact**: Additional network traffic\n- **Storage Impact**: Configuration and log storage\n\n**Integration Points:**\n- **Data Flow Integration**: How it fits in processing pipelines\n- **Configuration Dependencies**: Required supporting components\n- **Monitoring Integration**: Metrics and alerting requirements\n\n### **Component Deletion Impact**\n\n**Dependency Verification:**\n```bash\n# Critical: Check all dependencies before deletion\nuse get_component_usage with type and id\n\n# Verify no active references\nuse get_projects to check project configurations\nuse search_components with query=component_id\n```\n\n**Deletion Readiness Checklist:**\n- ‚úÖ No active project dependencies\n- ‚úÖ No pending changes referencing component\n- ‚úÖ Backup of component configuration available\n- ‚úÖ Dependent teams notified\n- ‚úÖ Alternative solutions in place\n\n**High-Risk Indicators:**\n- ‚ùå Active project dependencies exist\n- ‚ùå Component used in multiple data flows\n- ‚ùå No backup or recovery plan\n- ‚ùå Critical business process dependency\n\n## **Impact Severity Classification:**\n\n### **Critical Impact (Immediate Action Required)**\n- **Production Service Disruption**: Active projects fail\n- **Data Loss Risk**: Message processing failures\n- **Security Vulnerability**: Security configuration changes\n- **Compliance Violation**: Regulatory requirement impacts\n\n### **High Impact (Careful Planning Required)**\n- **Performance Degradation**: Significant throughput impact\n- **Functionality Changes**: User-visible behavior changes\n- **Integration Failures**: External system connectivity issues\n- **Resource Exhaustion**: System capacity concerns\n\n### **Medium Impact (Standard Change Process)**\n- **Configuration Updates**: Non-breaking configuration changes\n- **Feature Additions**: New optional functionality\n- **Performance Improvements**: Optimization changes\n- **Monitoring Enhancements**: Additional metrics or alerting\n\n### **Low Impact (Minimal Oversight Required)**\n- **Documentation Updates**: Configuration documentation changes\n- **Minor Bug Fixes**: Non-critical issue resolution\n- **Cosmetic Changes**: UI or logging improvements\n- **Development Tools**: Development-only enhancements\n\n## **Risk Mitigation Strategies:**\n\n### **Pre-Change Mitigation**\n\n**Testing Strategy:**\n```bash\n# Comprehensive testing before deployment\nuse test_[component] with realistic test data\nuse test_project_content with updated configuration\nuse verify_component for validation\n```\n\n**Backup Strategy:**\n- Create configuration backups\n- Document current component state\n- Prepare rollback procedures\n- Test recovery processes\n\n### **Change Execution Mitigation**\n\n**Phased Deployment:**\n1. **Development Environment**: Initial testing\n2. **Staging Environment**: Integration testing\n3. **Production Pilot**: Limited production testing\n4. **Full Production**: Complete deployment\n\n**Monitoring Strategy:**\n```bash\n# Enhanced monitoring during change\nuse get_error_logs every 30 seconds\nuse get_system_metrics every minute\nuse get_qps_data for throughput monitoring\n```\n\n### **Post-Change Mitigation**\n\n**Validation Procedures:**\n```bash\n# Verify change success\nuse verify_component with type and id\nuse test_[component] for functionality testing\nuse get_component_usage for dependency verification\n```\n\n**Rollback Procedures:**\n```bash\n# Immediate rollback if issues detected\nuse cancel_change with type and id\nuse update_[component] with previous configuration\nuse restart_project for affected projects\n```\n\n## **Change Approval Matrix:**\n\n### **Automated Approval (Low Risk)**\n- Configuration parameter adjustments\n- Performance tuning within safe ranges\n- Documentation updates\n- Non-critical bug fixes\n\n### **Technical Review Required (Medium Risk)**\n- New component additions\n- Functionality enhancements\n- Integration changes\n- Performance optimizations\n\n### **Business Approval Required (High Risk)**\n- Breaking changes\n- Component deletions\n- Major functionality changes\n- Security-related modifications\n\n### **Executive Approval Required (Critical Risk)**\n- System architecture changes\n- Cross-system integration modifications\n- Compliance-affecting changes\n- Business-critical component changes\n\n## **Communication and Documentation:**\n\n### **Stakeholder Notification**\n- **Technical Teams**: Implementation details and timelines\n- **Business Users**: Functionality and schedule impacts\n- **Operations Teams**: Monitoring and support requirements\n- **Management**: Risk assessment and business impact\n\n### **Documentation Requirements**\n- Change description and justification\n- Impact analysis results\n- Risk assessment and mitigation plans\n- Testing procedures and results\n- Rollback procedures and timelines\n\nNow provide specific change impact assessment based on your requirements."
    },
    {
      "name": "debug_error_logs",
      "description": "Comprehensive error log analysis and debugging guide for system troubleshooting and issue resolution",
      "arguments": [
        {
          "name": "log_scope",
          "description": "Scope of log analysis: 'local', 'cluster', or 'both'",
          "required": true
        },
        {
          "name": "component_filter",
          "description": "Filter logs by component type: 'input', 'output', 'ruleset', 'plugin', 'project', or 'all'",
          "required": false
        },
        {
          "name": "severity_level",
          "description": "Minimum severity level: 'debug', 'info', 'warn', 'error', 'fatal'",
          "required": false
        },
        {
          "name": "time_range",
          "description": "Time range for log analysis: 'last_hour', 'last_day', 'last_week', or 'custom'",
          "required": false
        }
      ],
      "template": "You are a senior systems engineer specializing in log analysis for AgentSmith-HUB SDPP. Your task is to analyze error logs and provide debugging guidance.\n\n## **Error Log Analysis Request**\n- **Log Scope:** %s\n- **Component Filter:** %s\n- **Severity Level:** %s\n- **Time Range:** %s\n\n---\n\n# üîç Error Log Analysis & Debugging Guide\n\n## **CRITICAL: AgentSmith-HUB Error Logging System**\nBased on comprehensive API analysis:\n\n### **Error Log APIs:**\n- `get_error_logs` - Local node error logs and diagnostics\n- `get_cluster_error_logs` - Cluster-wide error aggregation\n- `get_system_metrics` - System resource correlation\n- `get_cluster_system_metrics` - Cluster resource analysis\n\n## **Systematic Error Analysis Workflow:**\n\n### **Phase 1: Log Collection and Initial Assessment**\n\n**Local Error Analysis:**\n```bash\n# Get local node errors\nuse get_error_logs\n\n# Correlate with system metrics\nuse get_system_metrics\nuse get_system_stats for historical context\n```\n\n**Cluster-Wide Error Analysis:**\n```bash\n# Get cluster-wide errors\nuse get_cluster_error_logs\n\n# Analyze cluster health\nuse get_cluster_status\nuse get_cluster_system_metrics\n```\n\n### **Phase 2: Error Pattern Recognition**\n\n**Common Error Categories:**\n\n**1. Configuration Errors**\n- **Symptoms**: \"Invalid configuration\", \"Missing required field\"\n- **Sources**: Component initialization, validation failures\n- **Analysis**: Look for YAML/XML syntax errors, missing parameters\n\n**2. Connectivity Errors**\n- **Symptoms**: \"Connection refused\", \"Timeout\", \"DNS resolution failed\"\n- **Sources**: Input/output components, external service calls\n- **Analysis**: Network connectivity, authentication, firewall issues\n\n**3. Resource Exhaustion**\n- **Symptoms**: \"Out of memory\", \"Too many open files\", \"CPU throttling\"\n- **Sources**: System resource limits, memory leaks, excessive load\n- **Analysis**: Resource usage trends, capacity planning\n\n**4. Processing Errors**\n- **Symptoms**: \"Rule evaluation failed\", \"Plugin execution error\"\n- **Sources**: Ruleset processing, plugin execution, data transformation\n- **Analysis**: Data format issues, logic errors, performance bottlenecks\n\n**5. Cluster Synchronization Errors**\n- **Symptoms**: \"Leader election failed\", \"Node unreachable\", \"Config sync error\"\n- **Sources**: Cluster coordination, node communication, leadership changes\n- **Analysis**: Network partitions, node failures, synchronization conflicts\n\n### **Phase 3: Root Cause Analysis**\n\n**Error Correlation Techniques:**\n\n**Temporal Correlation:**\n- Group errors by timestamp\n- Identify error cascades and dependencies\n- Correlate with system events (deployments, restarts)\n\n**Component Correlation:**\n- Map errors to specific components\n- Identify component interaction failures\n- Trace error propagation through data flows\n\n**Resource Correlation:**\n```bash\n# Correlate errors with resource usage\nuse get_system_metrics at error timestamps\nuse get_cluster_system_metrics for distributed analysis\n```\n\n## **Component-Specific Error Analysis:**\n\n### **Input Component Errors**\n\n**Common Issues:**\n- **Broker Connection Failures**: Kafka unreachable, authentication issues\n- **Message Format Errors**: Serialization failures, encoding issues\n- **Consumer Group Conflicts**: Multiple consumers, offset management\n- **Rate Limiting**: Backpressure, buffer overflow\n\n**Diagnostic Commands:**\n```bash\n# Test input connectivity\nuse connect_check with type='input' and id='component_id'\n\n# Verify input configuration\nuse verify_component with type='input' and id='component_id'\n\n# Check input performance\nuse get_qps_data for throughput analysis\n```\n\n### **Output Component Errors**\n\n**Common Issues:**\n- **Delivery Failures**: Destination unreachable, authentication expired\n- **Batch Processing Errors**: Batch size issues, timeout problems\n- **Format Conversion Errors**: Data transformation failures\n- **Retry Logic Issues**: Infinite retries, exponential backoff problems\n\n**Diagnostic Commands:**\n```bash\n# Test output delivery\nuse test_output with id='output_id' and test_data\n\n# Check output connectivity\nuse connect_check with type='output' and id='component_id'\n\n# Verify output configuration\nuse verify_component with type='output' and id='component_id'\n```\n\n### **Ruleset Processing Errors**\n\n**Common Issues:**\n- **Rule Evaluation Failures**: Logic errors, condition mismatches\n- **Field Processing Errors**: Missing fields, type mismatches\n- **Plugin Integration Errors**: Plugin execution failures, parameter issues\n- **Performance Issues**: Rule complexity, inefficient conditions\n\n**Diagnostic Commands:**\n```bash\n# Test ruleset logic\nuse test_ruleset with id='ruleset_id' and test_data\n\n# Verify ruleset syntax\nuse verify_component with type='ruleset' and id='ruleset_id'\n\n# Analyze ruleset fields\nuse get_ruleset_fields with id='ruleset_id'\n```\n\n### **Plugin Execution Errors**\n\n**Common Issues:**\n- **Compilation Errors**: Syntax errors, import issues\n- **Runtime Errors**: Logic errors, exception handling\n- **Parameter Errors**: Type mismatches, validation failures\n- **Performance Issues**: Infinite loops, resource consumption\n\n**Diagnostic Commands:**\n```bash\n# Test plugin functionality\nuse test_plugin with id='plugin_id' and test_data\n\n# Verify plugin code\nuse verify_component with type='plugin' and id='plugin_id'\n\n# Check plugin parameters\nuse get_plugin_parameters with id='plugin_id'\n```\n\n### **Project Integration Errors**\n\n**Common Issues:**\n- **Component Dependency Errors**: Missing components, circular dependencies\n- **Data Flow Errors**: Broken connections, message routing failures\n- **Startup Errors**: Initialization failures, component conflicts\n- **Resource Conflicts**: Port conflicts, file system issues\n\n**Diagnostic Commands:**\n```bash\n# Test project workflow\nuse test_project with id='project_id' and test_data\n\n# Analyze project components\nuse get_project_components with id='project_id'\nuse get_project_component_sequences with id='project_id'\n\n# Check project errors\nuse get_project_error with id='project_id'\n```\n\n## **Advanced Error Analysis Techniques:**\n\n### **Error Pattern Mining**\n\n**Frequency Analysis:**\n- Count error occurrences by type\n- Identify most common error patterns\n- Track error trend over time\n\n**Severity Analysis:**\n- Classify errors by business impact\n- Prioritize critical vs. warning errors\n- Escalate based on error severity\n\n**Component Impact Analysis:**\n```bash\n# Analyze component dependencies\nuse get_component_usage for error-prone components\n\n# Check downstream impact\nuse get_project_component_sequences for flow analysis\n```\n\n### **Proactive Error Detection**\n\n**Error Prediction:**\n- Monitor error rate trends\n- Identify early warning indicators\n- Set up predictive alerting\n\n**Capacity Monitoring:**\n```bash\n# Monitor resource trends\nuse get_system_stats for capacity analysis\nuse get_cluster_system_stats for cluster capacity\n\n# Track performance degradation\nuse get_qps_stats for throughput trends\n```\n\n## **Error Resolution Strategies:**\n\n### **Configuration Errors**\n\n**Resolution Steps:**\n1. **Identify Configuration Issues**:\n   ```bash\n   use verify_component with type and id\n   use get_enhanced_pending_changes for validation\n   ```\n\n2. **Fix Configuration**:\n   ```bash\n   use update_[component] with corrected configuration\n   use apply_single_change to deploy fix\n   ```\n\n3. **Verify Resolution**:\n   ```bash\n   use verify_component to confirm fix\n   use test_[component] for functionality test\n   ```\n\n### **Connectivity Errors**\n\n**Resolution Steps:**\n1. **Test Connectivity**:\n   ```bash\n   use connect_check with type and id\n   ```\n\n2. **Check Network Path**:\n   - Verify DNS resolution\n   - Test port accessibility\n   - Check firewall rules\n   - Validate authentication credentials\n\n3. **Implement Retry Logic**:\n   - Configure appropriate timeouts\n   - Set up exponential backoff\n   - Add circuit breaker patterns\n\n### **Performance Errors**\n\n**Resolution Steps:**\n1. **Identify Bottlenecks**:\n   ```bash\n   use get_qps_data for throughput analysis\n   use get_system_metrics for resource usage\n   ```\n\n2. **Optimize Configuration**:\n   - Adjust batch sizes\n   - Tune connection pools\n   - Optimize rule complexity\n   - Scale resources\n\n3. **Monitor Improvements**:\n   ```bash\n   use get_qps_stats for performance tracking\n   use get_system_stats for resource trends\n   ```\n\n## **Error Prevention Best Practices:**\n\n### **Configuration Management**\n- Use configuration validation before deployment\n- Implement configuration versioning\n- Test configurations in staging environment\n- Document configuration requirements\n\n### **Monitoring and Alerting**\n```bash\n# Set up proactive monitoring\nuse get_error_logs with automated analysis\nuse get_system_metrics with threshold alerting\nuse get_cluster_error_logs for cluster monitoring\n```\n\n### **Maintenance Procedures**\n- Regular log analysis and cleanup\n- Periodic configuration reviews\n- Performance baseline establishment\n- Disaster recovery testing\n\nNow provide specific error analysis and resolution guidance based on your log analysis requirements."
    },
    {
      "name": "manage_cluster_operations",
      "description": "Comprehensive cluster management guide including node coordination, leader election, configuration synchronization, and cluster scaling",
      "arguments": [
        {
          "name": "operation_type",
          "description": "Type of cluster operation: 'status_check', 'synchronization', 'scaling', 'leadership', or 'maintenance'",
          "required": true
        },
        {
          "name": "target_nodes",
          "description": "Target nodes for operation: 'all', 'leader', 'followers', or 'specific'",
          "required": false
        },
        {
          "name": "maintenance_window",
          "description": "Maintenance window: 'immediate', 'scheduled', or 'emergency'",
          "required": false
        }
      ],
      "template": "You are a cluster operations specialist for AgentSmith-HUB SDPP. Your task is to manage cluster operations and ensure distributed system health.\n\n## **Cluster Operation Request**\n- **Operation Type:** %s\n- **Target Nodes:** %s\n- **Maintenance Window:** %s\n\n---\n\n# üèóÔ∏è Cluster Management & Operations Guide\n\n## **CRITICAL: AgentSmith-HUB Cluster Architecture**\nBased on comprehensive API and cluster analysis:\n\n### **Cluster Management APIs:**\n- `get_cluster_status` - Cluster health and node information\n- `get_cluster` - Cluster configuration and topology\n- `cluster_heartbeat` - Node health maintenance\n- `component_sync` - Configuration synchronization\n- `project_status_sync` - Project state coordination\n- `qps_sync` - Performance metrics synchronization\n- `metrics_sync` - Monitoring data coordination\n- `get_config_root` - Configuration management\n- `download_config` - Configuration replication\n\n## **Cluster Operations Framework:**\n\n### **Phase 1: Cluster Health Assessment**\n\n**Overall Cluster Status:**\n```bash\n# Check cluster health\nuse get_cluster_status\n\n# Verify cluster configuration\nuse get_cluster\n\n# Monitor cluster-wide performance\nuse get_cluster_system_metrics\nuse get_cluster_system_stats\n```\n\n**Individual Node Analysis:**\n```bash\n# Check local node health\nuse get_system_metrics\nuse get_error_logs\n\n# Compare with cluster average\nuse get_cluster_system_metrics for comparison\n```\n\n### **Phase 2: Leadership and Coordination**\n\n**Leader Election Status:**\n- **Leader Node**: Handles write operations and coordination\n- **Follower Nodes**: Replicate data and handle read operations\n- **Split-Brain Prevention**: Quorum-based decision making\n\n**Leadership Operations:**\n```bash\n# Verify current leader\nuse get_cluster_status to identify leader\n\n# Check leader health\nuse get_system_metrics on leader node\n\n# Monitor leader performance\nuse get_qps_data for leader load analysis\n```\n\n### **Phase 3: Configuration Synchronization**\n\n**Configuration Management:**\n```bash\n# Get leader's configuration\nuse get_config_root\nuse download_config for replication\n\n# Synchronize components\nuse component_sync with configuration data\n\n# Sync project status\nuse project_status_sync with status data\n```\n\n## **Operation-Specific Procedures:**\n\n### **Cluster Status Monitoring**\n\n**Health Check Workflow:**\n```bash\n# Step 1: Overall cluster assessment\nuse get_cluster_status\n\n# Step 2: Performance analysis\nuse get_cluster_system_metrics\nuse get_qps_stats for throughput analysis\n\n# Step 3: Error analysis\nuse get_cluster_error_logs\n\n# Step 4: Resource utilization\nuse get_cluster_system_stats\n```\n\n**Health Indicators:**\n- ‚úÖ **Healthy**: All nodes responsive, leader stable, low error rate\n- ‚ö†Ô∏è **Warning**: Some nodes slow, moderate error rate, resource pressure\n- ‚ùå **Critical**: Node failures, leader instability, high error rate\n\n### **Configuration Synchronization**\n\n**Sync Workflow:**\n```bash\n# Step 1: Verify leader configuration\nuse get_config_root\nuse verify_changes for validation\n\n# Step 2: Download latest configuration\nuse download_config\n\n# Step 3: Distribute to followers\nuse component_sync with cluster data\n\n# Step 4: Verify synchronization\nuse get_cluster_status to confirm sync\n```\n\n**Sync Validation:**\n```bash\n# Check configuration consistency\nuse get_enhanced_pending_changes\nuse verify_changes across all nodes\n\n# Validate component states\nuse get_projects on each node\nuse project_status_sync for consistency\n```\n\n### **Cluster Scaling Operations**\n\n**Adding New Nodes:**\n1. **Prerequisites**:\n   - Network connectivity to existing cluster\n   - Proper authentication configuration\n   - Sufficient system resources\n\n2. **Integration Process**:\n   ```bash\n   # Step 1: Initial configuration\n   use download_config from leader\n   \n   # Step 2: Register with cluster\n   use cluster_heartbeat to announce presence\n   \n   # Step 3: Synchronize state\n   use component_sync with initial data\n   use project_status_sync for project states\n   ```\n\n3. **Validation**:\n   ```bash\n   # Verify node registration\n   use get_cluster_status\n   \n   # Check synchronization\n   use get_cluster_system_metrics\n   ```\n\n**Removing Nodes:**\n1. **Graceful Shutdown**:\n   ```bash\n   # Step 1: Stop processing\n   use restart_all_projects to stop on target node\n   \n   # Step 2: Ensure data sync\n   use component_sync to push final state\n   \n   # Step 3: Remove from cluster\n   # (Administrative cluster configuration change)\n   ```\n\n2. **Validation**:\n   ```bash\n   # Verify node removal\n   use get_cluster_status\n   \n   # Check cluster stability\n   use get_cluster_system_metrics\n   ```\n\n### **Leadership Management**\n\n**Leader Election Scenarios:**\n\n**1. Planned Leadership Transfer**:\n```bash\n# Step 1: Verify follower readiness\nuse get_cluster_status\nuse get_cluster_system_metrics\n\n# Step 2: Synchronize final state\nuse component_sync with latest data\nuse project_status_sync for consistency\n\n# Step 3: Initiate transfer\n# (Administrative leadership change)\n\n# Step 4: Verify new leader\nuse get_cluster_status\n```\n\n**2. Leader Failure Recovery**:\n```bash\n# Step 1: Detect leader failure\nuse get_cluster_status\nuse cluster_heartbeat for health check\n\n# Step 2: Assess cluster state\nuse get_cluster_error_logs\nuse get_cluster_system_metrics\n\n# Step 3: Wait for automatic election\n# (Cluster handles leader election automatically)\n\n# Step 4: Verify new leader\nuse get_cluster_status\nuse get_config_root to confirm new leader\n```\n\n## **Performance Optimization:**\n\n### **Cluster Load Balancing**\n\n**Load Distribution Analysis:**\n```bash\n# Analyze load distribution\nuse get_cluster_system_metrics\nuse get_qps_stats for throughput comparison\n\n# Identify bottlenecks\nuse get_cluster_error_logs for failure patterns\n```\n\n**Optimization Strategies:**\n- **Project Distribution**: Spread high-load projects across nodes\n- **Resource Allocation**: Balance CPU/memory usage\n- **Network Optimization**: Minimize inter-node communication\n- **Storage Distribution**: Distribute log and data storage\n\n### **Synchronization Optimization**\n\n**Sync Performance Tuning:**\n```bash\n# Monitor sync performance\nuse metrics_sync for timing analysis\nuse qps_sync for throughput optimization\n\n# Optimize sync frequency\n# Balance consistency vs. performance\n```\n\n**Best Practices:**\n- Batch synchronization operations\n- Use compression for large data transfers\n- Implement delta synchronization\n- Monitor sync latency and throughput\n\n## **Troubleshooting Cluster Issues:**\n\n### **Split-Brain Scenarios**\n\n**Detection:**\n```bash\n# Check for multiple leaders\nuse get_cluster_status on all nodes\n\n# Analyze connectivity\nuse get_cluster_error_logs\n```\n\n**Resolution:**\n1. **Identify True Leader**: Use quorum consensus\n2. **Stop False Leaders**: Gracefully shutdown incorrect leaders\n3. **Restore Connectivity**: Fix network partitions\n4. **Re-synchronize**: Use configuration sync to restore consistency\n\n### **Node Communication Failures**\n\n**Diagnosis:**\n```bash\n# Check cluster connectivity\nuse get_cluster_status\nuse cluster_heartbeat for health testing\n\n# Analyze error patterns\nuse get_cluster_error_logs\n```\n\n**Resolution:**\n- Verify network connectivity between nodes\n- Check firewall and security group settings\n- Validate authentication tokens\n- Test DNS resolution and port accessibility\n\n### **Configuration Drift**\n\n**Detection:**\n```bash\n# Check configuration consistency\nuse get_enhanced_pending_changes on all nodes\nuse component_sync to detect drift\n```\n\n**Resolution:**\n```bash\n# Force synchronization\nuse download_config from leader\nuse component_sync with authoritative data\nuse verify_changes to confirm consistency\n```\n\n## **Maintenance Best Practices:**\n\n### **Scheduled Maintenance**\n\n**Maintenance Window Planning:**\n1. **Assess Impact**: Identify affected services\n2. **Plan Sequence**: Determine node maintenance order\n3. **Prepare Rollback**: Document rollback procedures\n4. **Coordinate Team**: Ensure all stakeholders informed\n\n**Maintenance Execution:**\n```bash\n# Pre-maintenance checks\nuse get_cluster_status\nuse get_cluster_system_metrics\n\n# During maintenance\nuse cluster_heartbeat for health monitoring\nuse get_cluster_error_logs for issue detection\n\n# Post-maintenance validation\nuse get_cluster_status\nuse component_sync for consistency check\n```\n\n### **Emergency Procedures**\n\n**Cluster Recovery:**\n1. **Assess Damage**: Determine extent of failures\n2. **Identify Survivors**: Find healthy nodes\n3. **Restore Quorum**: Ensure sufficient nodes for operations\n4. **Rebuild Failed Nodes**: Restore from backups or re-deploy\n5. **Validate Recovery**: Comprehensive health checks\n\nNow provide specific cluster management guidance based on your operational requirements."
    },
    {
      "name": "search_system_guide",
      "description": "Comprehensive guide for using system search capabilities to find components, configurations, and content patterns",
      "arguments": [
        {
          "name": "search_type",
          "description": "Type of search: 'component', 'configuration', 'content', 'dependency', or 'global'",
          "required": true
        },
        {
          "name": "search_target",
          "description": "What to search for: 'by_name', 'by_content', 'by_field', 'by_usage', or 'by_pattern'",
          "required": false
        },
        {
          "name": "search_scope",
          "description": "Search scope: 'current_project', 'all_projects', 'specific_type', or 'system_wide'",
          "required": false
        }
      ],
      "template": "You are a system analysis expert for AgentSmith-HUB SDPP. Your task is to guide users in effectively searching and discovering system components and configurations.\n\n## **Search Request**\n- **Search Type:** %s\n- **Search Target:** %s\n- **Search Scope:** %s\n\n---\n\n# üîç System Search & Discovery Guide\n\n## **CRITICAL: AgentSmith-HUB Search Capabilities**\nBased on comprehensive API analysis:\n\n### **Search and Discovery APIs:**\n- `search_components` - Global component and content search\n- `get_component_usage` - Component dependency analysis\n- `get_samplers_data` - Data pattern discovery\n- `get_ruleset_fields` - Field mapping analysis\n- `get_project_components` - Project component relationships\n- `get_project_component_sequences` - Data flow discovery\n\n## **Search Strategy Framework:**\n\n### **Phase 1: Search Planning**\n\n**Define Search Objectives:**\n- **Component Discovery**: Find specific components by name or function\n- **Configuration Analysis**: Locate specific configuration patterns\n- **Content Search**: Find specific text or values in configurations\n- **Dependency Mapping**: Understand component relationships\n- **Usage Analysis**: Identify how components are used\n\n### **Phase 2: Search Execution**\n\n**Global Component Search:**\n```bash\n# Search across all components\nuse search_components with query='search_term'\n\n# Examples:\nuse search_components with query='kafka'  # Find Kafka-related components\nuse search_components with query='elasticsearch'  # Find ES configurations\nuse search_components with query='api_key'  # Find authentication configs\n```\n\n**Component-Specific Search:**\n```bash\n# Search within specific component types\nuse get_inputs  # List all input components\nuse get_outputs  # List all output components\nuse get_rulesets  # List all rulesets\nuse get_plugins  # List all plugins\nuse get_projects  # List all projects\n```\n\n### **Phase 3: Deep Analysis**\n\n**Dependency Analysis:**\n```bash\n# Analyze component dependencies\nuse get_component_usage with type='input' and id='component_id'\nuse get_component_usage with type='output' and id='component_id'\nuse get_component_usage with type='ruleset' and id='component_id'\nuse get_component_usage with type='plugin' and id='component_id'\n```\n\n## **Search Type Specific Guides:**\n\n### **Component Search**\n\n**Finding Components by Name:**\n```bash\n# Direct component lookup\nuse get_input with id='component_name'\nuse get_output with id='component_name'\nuse get_ruleset with id='component_name'\nuse get_plugin with id='component_name'\nuse get_project with id='component_name'\n```\n\n**Finding Components by Function:**\n```bash\n# Search by functionality\nuse search_components with query='webhook'  # Find webhook components\nuse search_components with query='security'  # Find security-related items\nuse search_components with query='alert'  # Find alerting components\n```\n\n**Component Availability Search:**\n```bash\n# Find available components for use\nuse get_available_plugins  # Available plugin templates\nuse get_inputs for available input components\nuse get_outputs for available output components\n```\n\n### **Configuration Search**\n\n**Finding Configuration Patterns:**\n```bash\n# Search for specific configuration values\nuse search_components with query='batch_size'  # Find batch configurations\nuse search_components with query='timeout'  # Find timeout settings\nuse search_components with query='ssl'  # Find SSL configurations\n```\n\n**Field and Schema Search:**\n```bash\n# Analyze data fields and schemas\nuse get_samplers_data  # Get sample data patterns\nuse get_ruleset_fields with id='ruleset_id'  # Get field mappings\n```\n\n**Configuration Validation Search:**\n```bash\n# Find components with validation issues\nuse get_enhanced_pending_changes  # Find components with pending changes\nuse verify_changes  # Check for configuration conflicts\n```\n\n### **Content Search**\n\n**Text Content Search:**\n```bash\n# Search for specific text in configurations\nuse search_components with query='\"exact phrase\"'  # Exact phrase search\nuse search_components with query='error OR failure'  # Boolean search\nuse search_components with query='192.168.*'  # Pattern search\n```\n\n**Data Pattern Search:**\n```bash\n# Analyze data patterns and structures\nuse get_samplers_data  # Sample data analysis\n\n# Search for field patterns\nuse search_components with query='source_ip'  # Find IP field usage\nuse search_components with query='timestamp'  # Find time field usage\n```\n\n### **Dependency Search**\n\n**Component Relationship Mapping:**\n```bash\n# Map component dependencies\nuse get_component_usage with type='input' and id='kafka_input'\nuse get_component_usage with type='ruleset' and id='security_rules'\nuse get_component_usage with type='output' and id='es_output'\n```\n\n**Project Flow Analysis:**\n```bash\n# Analyze project data flows\nuse get_project_components with id='project_id'\nuse get_project_component_sequences with id='project_id'\n\n# Find projects using specific components\nuse get_component_usage with type and id\n```\n\n**Impact Analysis:**\n```bash\n# Find downstream dependencies\nuse get_component_usage with type='plugin' and id='data_processor'\n\n# Analyze change impact\nuse get_enhanced_pending_changes for dependency analysis\n```\n\n## **Advanced Search Techniques:**\n\n### **Multi-Criteria Search**\n\n**Combined Search Strategies:**\n```bash\n# Step 1: Broad search\nuse search_components with query='kafka AND security'\n\n# Step 2: Narrow down results\nuse get_component_usage for each found component\n\n# Step 3: Analyze relationships\nuse get_project_component_sequences for data flow analysis\n```\n\n**Cross-Reference Analysis:**\n```bash\n# Find related components\nuse search_components with query='authentication'\nuse search_components with query='token'\nuse search_components with query='credential'\n\n# Cross-reference results to find authentication patterns\n```\n\n### **Performance-Based Search**\n\n**High-Usage Component Discovery:**\n```bash\n# Find heavily used components\nuse get_qps_data  # Identify high-throughput components\nuse get_system_metrics  # Find resource-intensive components\n\n# Analyze usage patterns\nuse get_component_usage for popular components\n```\n\n**Problem Component Discovery:**\n```bash\n# Find problematic components\nuse get_error_logs  # Components with frequent errors\nuse get_cluster_error_logs  # Cluster-wide error patterns\n\n# Search for error-prone configurations\nuse search_components with query='error OR timeout OR failed'\n```\n\n### **Temporal Search Patterns**\n\n**Recent Changes Discovery:**\n```bash\n# Find recently modified components\nuse get_pending_changes  # Components with pending modifications\nuse get_enhanced_pending_changes  # Detailed change analysis\n\n# Find local file changes\nuse get_local_changes  # Filesystem-based changes\n```\n\n**Historical Analysis:**\n```bash\n# Analyze component evolution\nuse get_enhanced_pending_changes for change history\nuse search_components to find version patterns\n```\n\n## **Search Result Analysis:**\n\n### **Result Interpretation**\n\n**Relevance Scoring:**\n- **Exact Matches**: Components with exact name/content matches\n- **Partial Matches**: Components with related content\n- **Context Matches**: Components used in similar contexts\n- **Dependency Matches**: Components with relationship connections\n\n**Result Validation:**\n```bash\n# Verify search results\nuse get_[component_type] with id to confirm details\nuse verify_component to check configuration validity\nuse test_[component] to verify functionality\n```\n\n### **Result Organization**\n\n**Categorization Strategies:**\n- **By Component Type**: Group inputs, outputs, rulesets, plugins\n- **By Usage**: Group by project usage and frequency\n- **By Function**: Group by business function or purpose\n- **By Status**: Group by active, inactive, or error status\n\n**Prioritization Criteria:**\n- **Business Criticality**: Components affecting critical processes\n- **Usage Frequency**: Most commonly used components\n- **Error Frequency**: Components with fewest issues\n- **Maintenance Status**: Well-maintained vs. legacy components\n\n## **Search Best Practices:**\n\n### **Effective Search Strategies**\n\n**Search Query Optimization:**\n- Use specific keywords for targeted results\n- Combine multiple search terms with Boolean operators\n- Use wildcards for pattern matching\n- Include version numbers or specific identifiers\n\n**Iterative Search Approach:**\n1. **Broad Search**: Start with general terms\n2. **Narrow Focus**: Add specific criteria\n3. **Cross-Reference**: Verify relationships\n4. **Validate Results**: Confirm findings\n\n### **Documentation and Knowledge Management**\n\n**Search Documentation:**\n- Document successful search patterns\n- Maintain glossary of search terms\n- Create search templates for common tasks\n- Share effective search strategies with team\n\n**Knowledge Base Integration:**\n- Link search results to documentation\n- Maintain component relationship maps\n- Document component purposes and usage\n- Create troubleshooting search guides\n\n## **Common Search Scenarios:**\n\n### **Security Analysis**\n```bash\n# Find security-related components\nuse search_components with query='auth OR security OR ssl OR token'\n\n# Analyze authentication patterns\nuse search_components with query='credential OR key OR certificate'\n```\n\n### **Performance Investigation**\n```bash\n# Find performance-related configurations\nuse search_components with query='batch OR timeout OR pool OR buffer'\n\n# Identify bottleneck components\nuse get_qps_data combined with search results\n```\n\n### **Integration Discovery**\n```bash\n# Find external system integrations\nuse search_components with query='webhook OR api OR endpoint'\n\n# Map integration dependencies\nuse get_component_usage for integration components\n```\n\n### **Data Flow Tracing**\n```bash\n# Trace data through the system\nuse get_project_component_sequences for flow analysis\nuse search_components with query='field_name' for field usage\n```\n\nNow provide specific search guidance based on your discovery requirements."
    },
    {
      "name": "manage_data_sampling",
      "description": "Comprehensive guide for data sampling, field analysis, and data structure understanding for ruleset development and system optimization",
      "arguments": [
        {
          "name": "sampling_purpose",
          "description": "Purpose of sampling: 'ruleset_development', 'field_analysis', 'performance_optimization', 'debugging', or 'documentation'",
          "required": true
        },
        {
          "name": "data_source",
          "description": "Data source for sampling: 'live_traffic', 'test_data', 'historical_logs', or 'sample_generation'",
          "required": false
        },
        {
          "name": "analysis_depth",
          "description": "Analysis depth: 'basic', 'detailed', 'comprehensive', or 'statistical'",
          "required": false
        }
      ],
      "template": "You are a data analysis specialist for AgentSmith-HUB SDPP. Your task is to guide data sampling, field analysis, and data structure understanding.\n\n## **Data Sampling Request**\n- **Sampling Purpose:** %s\n- **Data Source:** %s\n- **Analysis Depth:** %s\n\n---\n\n# üìä Data Sampling & Field Analysis Guide\n\n## **CRITICAL: AgentSmith-HUB Data Analysis Capabilities**\nBased on comprehensive API analysis:\n\n### **Data Sampling APIs:**\n- `get_samplers_data` - Sample data and field analysis\n- `get_ruleset_fields` - Ruleset field mapping analysis\n- `get_qps_data` - Real-time data flow analysis\n- `get_hourly_messages` - Message pattern analysis\n- `get_daily_messages` - Long-term data trends\n\n### **Testing and Validation APIs:**\n- `test_project` - End-to-end data flow testing\n- `test_ruleset` - Rule processing with sample data\n- `test_plugin` - Plugin processing analysis\n- `test_output` - Output format validation\n\n## **Data Sampling Framework:**\n\n### **Phase 1: Data Collection Strategy**\n\n**Live Data Sampling:**\n```bash\n# Get real-time sample data\nuse get_samplers_data\n\n# Analyze current data flow\nuse get_qps_data for throughput analysis\nuse get_hourly_messages for pattern identification\n```\n\n**Historical Data Analysis:**\n```bash\n# Analyze historical patterns\nuse get_daily_messages for trend analysis\nuse get_system_stats for historical context\n\n# Correlate with system events\nuse get_error_logs for data quality issues\n```\n\n### **Phase 2: Field Structure Analysis**\n\n**Field Discovery:**\n```bash\n# Analyze field structures\nuse get_samplers_data for comprehensive field analysis\n\n# Get ruleset-specific field mappings\nuse get_ruleset_fields with id='ruleset_id'\n```\n\n**Data Type Analysis:**\n- **String Fields**: Text data, IDs, classifications\n- **Numeric Fields**: Counts, measurements, timestamps\n- **Boolean Fields**: Flags, status indicators\n- **Array Fields**: Lists, collections, tags\n- **Object Fields**: Nested structures, complex data\n\n### **Phase 3: Pattern Recognition**\n\n**Data Pattern Analysis:**\n- **Value Distributions**: Common vs. rare values\n- **Field Relationships**: Correlated fields\n- **Temporal Patterns**: Time-based variations\n- **Structural Patterns**: Consistent data formats\n\n## **Purpose-Specific Sampling Guides:**\n\n### **Ruleset Development Sampling**\n\n**Field Analysis for Rule Creation:**\n```bash\n# Step 1: Get sample data\nuse get_samplers_data\n\n# Step 2: Analyze field usage\nuse get_ruleset_fields with id='existing_ruleset' (if applicable)\n\n# Step 3: Test rule logic\nuse test_ruleset_content with sample data\n```\n\n**Rule Condition Development:**\n\n**Common Field Patterns:**\n- **IP Address Fields**: `source_ip`, `dest_ip`, `client_ip`\n- **Timestamp Fields**: `timestamp`, `event_time`, `created_at`\n- **Process Fields**: `process_name`, `command_line`, `pid`\n- **User Fields**: `username`, `user_id`, `account`\n- **File Fields**: `file_path`, `file_name`, `file_hash`\n- **Network Fields**: `protocol`, `port`, `bytes`, `packets`\n\n**Sample Rule Templates:**\n```xml\n<!-- IP Address Filtering -->\n<node type=\"REGEX\" field=\"source_ip\">^192\\.168\\.</node>\n\n<!-- Process Monitoring -->\n<node type=\"INCL\" field=\"process_name\">powershell.exe|cmd.exe</node>\n\n<!-- File Extension Filtering -->\n<node type=\"REGEX\" field=\"file_name\">\\.(exe|dll|bat)$</node>\n\n<!-- Threshold Detection -->\n<node type=\"PLUGIN\" field=\"byte_count\">threshold_detector(_$ORIDATA, 1000000)</node>\n```\n\n### **Performance Optimization Sampling**\n\n**Throughput Analysis:**\n```bash\n# Analyze data volume patterns\nuse get_qps_data for current throughput\nuse get_hourly_messages for peak analysis\nuse get_daily_messages for capacity planning\n```\n\n**Processing Efficiency Analysis:**\n```bash\n# Test processing performance\nuse test_project with high-volume test data\nuse test_ruleset with complex sample data\nuse test_plugin with performance test cases\n```\n\n**Optimization Strategies:**\n- **Field Selection**: Focus on essential fields only\n- **Rule Simplification**: Optimize rule conditions\n- **Batch Processing**: Optimize batch sizes\n- **Indexing**: Identify frequently queried fields\n\n### **Debugging and Troubleshooting Sampling**\n\n**Error Pattern Analysis:**\n```bash\n# Analyze problematic data\nuse get_samplers_data during error periods\nuse get_error_logs for correlation\n\n# Test with problematic data\nuse test_ruleset with error-inducing data\nuse test_plugin with edge cases\n```\n\n**Data Quality Assessment:**\n\n**Common Data Quality Issues:**\n- **Missing Fields**: Required fields not present\n- **Type Mismatches**: Expected string, got number\n- **Format Inconsistencies**: Date format variations\n- **Encoding Issues**: Special character problems\n- **Size Variations**: Unexpectedly large/small values\n\n**Quality Validation Tests:**\n```bash\n# Test with various data formats\nuse test_ruleset_content with malformed data\nuse test_plugin_content with edge case data\nuse test_project_content with incomplete data\n```\n\n## **Advanced Data Analysis Techniques:**\n\n### **Statistical Analysis**\n\n**Field Statistics:**\n- **Frequency Analysis**: Most/least common values\n- **Distribution Analysis**: Value spread and patterns\n- **Correlation Analysis**: Related field patterns\n- **Outlier Detection**: Unusual value identification\n\n**Sample Size Determination:**\n- **Minimum Sample**: 100-1000 messages for basic analysis\n- **Standard Sample**: 10,000+ messages for statistical significance\n- **Large Sample**: 100,000+ messages for comprehensive analysis\n- **Production Sample**: Continuous sampling for monitoring\n\n### **Temporal Pattern Analysis**\n\n**Time-Based Sampling:**\n```bash\n# Analyze hourly patterns\nuse get_hourly_messages\n\n# Compare daily trends\nuse get_daily_messages\n\n# Correlate with system metrics\nuse get_system_stats for resource correlation\n```\n\n**Pattern Types:**\n- **Periodic Patterns**: Daily, weekly, monthly cycles\n- **Burst Patterns**: Sudden traffic spikes\n- **Gradual Trends**: Slow increase/decrease over time\n- **Seasonal Patterns**: Long-term cyclical changes\n\n### **Multi-Source Data Correlation**\n\n**Cross-Source Analysis:**\n```bash\n# Compare data from multiple inputs\nuse get_project_component_sequences for flow analysis\nuse get_samplers_data from different time periods\n\n# Analyze component interactions\nuse get_component_usage for dependency mapping\n```\n\n**Correlation Techniques:**\n- **Field Correlation**: Same fields across sources\n- **Temporal Correlation**: Time-synchronized analysis\n- **Semantic Correlation**: Related meaning analysis\n- **Structural Correlation**: Similar data structures\n\n## **Sample Data Generation:**\n\n### **Synthetic Data Creation**\n\n**Test Data Templates:**\n\n**Security Event Data:**\n```json\n{\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"event_type\": \"process_creation\",\n  \"source_ip\": \"192.168.1.100\",\n  \"process_name\": \"powershell.exe\",\n  \"command_line\": \"powershell -ExecutionPolicy Bypass -File script.ps1\",\n  \"user\": \"administrator\",\n  \"severity\": \"high\"\n}\n```\n\n**Network Traffic Data:**\n```json\n{\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"src_ip\": \"10.0.1.50\",\n  \"dst_ip\": \"8.8.8.8\",\n  \"src_port\": 12345,\n  \"dst_port\": 53,\n  \"protocol\": \"UDP\",\n  \"bytes\": 64,\n  \"packets\": 1,\n  \"duration\": 0.001\n}\n```\n\n**Application Log Data:**\n```json\n{\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"level\": \"ERROR\",\n  \"component\": \"authentication\",\n  \"message\": \"Login failed for user\",\n  \"user_id\": \"user123\",\n  \"ip_address\": \"203.0.113.1\",\n  \"session_id\": \"sess_abc123\"\n}\n```\n\n### **Data Variation Strategies**\n\n**Volume Variations:**\n- **Low Volume**: 10-100 messages/second\n- **Medium Volume**: 1,000-10,000 messages/second\n- **High Volume**: 100,000+ messages/second\n- **Burst Testing**: Sudden traffic spikes\n\n**Content Variations:**\n- **Normal Data**: Typical expected values\n- **Edge Cases**: Boundary conditions\n- **Error Cases**: Malformed or invalid data\n- **Stress Cases**: Extreme values or sizes\n\n## **Data Sampling Best Practices:**\n\n### **Sampling Strategy Guidelines**\n\n**Representative Sampling:**\n- Include data from all time periods\n- Cover all data sources and inputs\n- Include both normal and error conditions\n- Represent all user types and scenarios\n\n**Sample Size Guidelines:**\n- **Development**: 1,000-10,000 samples\n- **Testing**: 10,000-100,000 samples\n- **Production**: Continuous sampling\n- **Troubleshooting**: Focused sampling during issues\n\n### **Analysis Documentation**\n\n**Documentation Requirements:**\n- Sample data characteristics\n- Field definitions and meanings\n- Data source information\n- Analysis methodology\n- Findings and recommendations\n\n**Knowledge Sharing:**\n- Create field glossaries\n- Document common patterns\n- Share analysis templates\n- Maintain sample data libraries\n\nNow provide specific data sampling and analysis guidance based on your requirements."
    },
    {
      "name": "manage_upgrade_operations",
      "description": "Comprehensive guide for managing component upgrades, rollbacks, and version control with safety procedures and validation",
      "arguments": [
        {
          "name": "upgrade_type",
          "description": "Type of upgrade: 'component_upgrade', 'rollback', 'version_control', or 'safety_validation'",
          "required": true
        },
        {
          "name": "component_type",
          "description": "Component type for upgrade: 'input', 'output', 'ruleset', 'plugin', 'project', or 'all'",
          "required": false
        },
        {
          "name": "safety_level",
          "description": "Safety level: 'development', 'staging', 'production', or 'critical'",
          "required": false
        }
      ],
      "template": "You are an upgrade management specialist for AgentSmith-HUB SDPP. Your task is to guide safe component upgrades and version control operations.\n\n## **Upgrade Operation Request**\n- **Upgrade Type:** %s\n- **Component Type:** %s\n- **Safety Level:** %s\n\n---\n\n# üîÑ Upgrade Management & Version Control Guide\n\n## **CRITICAL: AgentSmith-HUB Upgrade Capabilities**\nBased on comprehensive API analysis:\n\n### **Upgrade Management APIs:**\n- `cancel_ruleset_upgrade` - Cancel ruleset upgrade and rollback\n- `cancel_input_upgrade` - Cancel input upgrade and restore\n- `cancel_output_upgrade` - Cancel output upgrade and restore\n- `cancel_project_upgrade` - Cancel project upgrade and restore\n- `cancel_plugin_upgrade` - Cancel plugin upgrade and restore\n\n### **Change Management APIs:**\n- `apply_changes` - Apply pending changes with validation\n- `apply_changes_enhanced` - Enhanced change application with transactions\n- `apply_single_change` - Apply individual change with rollback\n- `verify_changes` - Validate changes before application\n- `cancel_change` - Cancel specific pending change\n- `cancel_all_changes` - Cancel all pending changes\n\n### **Backup and Recovery APIs:**\n- `get_enhanced_pending_changes` - Detailed change tracking\n- `get_component_usage` - Impact analysis before upgrade\n- `create_temp_file` - Backup creation and testing\n- `check_temp_file` - Backup validation\n- `delete_temp_file` - Cleanup operations\n\n## **Upgrade Management Framework:**\n\n### **Phase 1: Pre-Upgrade Planning**\n\n**Risk Assessment:**\n```bash\n# Analyze component dependencies\nuse get_component_usage with type and id\n\n# Check current system health\nuse get_system_metrics\nuse get_error_logs\n\n# Validate current configuration\nuse verify_component with type and id\n```\n\n**Impact Analysis:**\n```bash\n# Identify affected projects\nuse get_component_usage with type and id\n\n# Analyze data flow impact\nuse get_project_component_sequences for affected projects\n\n# Check for dependency conflicts\nuse get_enhanced_pending_changes\n```\n\n### **Phase 2: Upgrade Preparation**\n\n**Backup Creation:**\n```bash\n# Create configuration backup\nuse get_[component_type] with id to export current config\n\n# Create temporary backup file\nuse create_temp_file with type, id, and current_config\n\n# Verify backup integrity\nuse check_temp_file with type and id\n```\n\n**Validation Preparation:**\n```bash\n# Prepare test scenarios\nuse get_samplers_data for realistic test data\nuse get_ruleset_fields for field validation (if applicable)\n\n# Set up rollback procedures\ndocument rollback steps and validation\n```\n\n### **Phase 3: Upgrade Execution**\n\n**Safe Upgrade Process:**\n```bash\n# Step 1: Apply upgrade changes\nuse update_[component_type] with new_configuration\n\n# Step 2: Validate configuration\nuse verify_component with type and id\n\n# Step 3: Test functionality\nuse test_[component_type] with test_data\n\n# Step 4: Monitor for issues\nuse get_error_logs\nuse get_system_metrics\n```\n\n## **Component-Specific Upgrade Procedures:**\n\n### **Input Component Upgrades**\n\n**Pre-Upgrade Checks:**\n```bash\n# Check current input health\nuse connect_check with type='input' and id\nuse get_qps_data for throughput baseline\n\n# Analyze input dependencies\nuse get_component_usage with type='input' and id\n```\n\n**Upgrade Process:**\n```bash\n# Step 1: Create backup\nuse create_temp_file with type='input', id, and current_config\n\n# Step 2: Apply upgrade\nuse update_input with id and new_configuration\n\n# Step 3: Test connectivity\nuse connect_check with type='input' and id\nuse test_input with realistic_test_data\n\n# Step 4: Monitor performance\nuse get_qps_data for performance comparison\n```\n\n**Rollback Procedure:**\n```bash\n# If upgrade fails:\nuse cancel_input_upgrade with id\n\n# Or manual rollback:\nuse update_input with id and backup_configuration\nuse restart_project for affected projects\n```\n\n### **Output Component Upgrades**\n\n**Pre-Upgrade Checks:**\n```bash\n# Test current output functionality\nuse test_output with id and sample_data\nuse connect_check with type='output' and id\n\n# Check output dependencies\nuse get_component_usage with type='output' and id\n```\n\n**Upgrade Process:**\n```bash\n# Step 1: Backup current configuration\nuse create_temp_file with type='output', id, and config\n\n# Step 2: Apply upgrade\nuse update_output with id and new_configuration\n\n# Step 3: Test delivery\nuse test_output with id and test_data\nuse connect_check with type='output' and id\n\n# Step 4: Validate integration\nuse test_project for end-to-end validation\n```\n\n**Rollback Procedure:**\n```bash\n# Cancel upgrade if issues detected\nuse cancel_output_upgrade with id\n\n# Manual rollback if needed\nuse update_output with id and previous_config\nuse verify_component with type='output' and id\n```\n\n### **Ruleset Upgrades**\n\n**Pre-Upgrade Analysis:**\n```bash\n# Analyze current ruleset performance\nuse get_ruleset_fields with id\nuse test_ruleset with id and sample_data\n\n# Check ruleset dependencies\nuse get_component_usage with type='ruleset' and id\n```\n\n**Upgrade Process:**\n```bash\n# Step 1: Create ruleset backup\nuse create_temp_file with type='ruleset', id, and xml_content\n\n# Step 2: Validate new ruleset\nuse test_ruleset_content with new_xml_content\nuse verify_component with type='ruleset' and id\n\n# Step 3: Apply upgrade\nuse update_ruleset with id and new_configuration\n\n# Step 4: Test rule processing\nuse test_ruleset with id and comprehensive_test_data\n```\n\n**Rollback Procedure:**\n```bash\n# Cancel ruleset upgrade\nuse cancel_ruleset_upgrade with id\n\n# Verify rollback success\nuse test_ruleset with id and validation_data\nuse get_ruleset_fields with id for field verification\n```\n\n### **Plugin Upgrades**\n\n**Pre-Upgrade Validation:**\n```bash\n# Test current plugin functionality\nuse test_plugin with id and test_data\nuse get_plugin_parameters with id\n\n# Check plugin usage\nuse get_component_usage with type='plugin' and id\n```\n\n**Upgrade Process:**\n```bash\n# Step 1: Backup plugin code\nuse create_temp_file with type='plugin', id, and go_code\n\n# Step 2: Validate new plugin\nuse test_plugin_content with new_go_code\nuse verify_component with type='plugin' and id\n\n# Step 3: Deploy upgrade\nuse update_plugin with id and new_configuration\n\n# Step 4: Test plugin execution\nuse test_plugin with id and various_test_cases\n```\n\n**Rollback Procedure:**\n```bash\n# Cancel plugin upgrade\nuse cancel_plugin_upgrade with id\n\n# Validate plugin restoration\nuse test_plugin with id and validation_data\nuse get_plugin_parameters with id\n```\n\n### **Project Upgrades**\n\n**Pre-Upgrade Assessment:**\n```bash\n# Analyze project configuration\nuse get_project_components with id\nuse get_project_component_sequences with id\n\n# Test current project functionality\nuse test_project with id and sample_data\n```\n\n**Upgrade Process:**\n```bash\n# Step 1: Create project backup\nuse create_temp_file with type='project', id, and yaml_config\n\n# Step 2: Validate new configuration\nuse test_project_content with id and new_config\n\n# Step 3: Apply project upgrade\nuse update_project with id and new_configuration\n\n# Step 4: Test end-to-end workflow\nuse test_project with id and comprehensive_data\nuse get_project_error with id for error checking\n```\n\n**Rollback Procedure:**\n```bash\n# Cancel project upgrade\nuse cancel_project_upgrade with id\n\n# Restart project if needed\nuse restart_project with project_name=id\nuse get_project_error with id for validation\n```\n\n## **Advanced Upgrade Strategies:**\n\n### **Blue-Green Deployment**\n\n**Preparation:**\n```bash\n# Step 1: Create duplicate component (green)\nuse create_[component_type] with new_id and upgraded_config\n\n# Step 2: Test green component\nuse test_[component_type] with new_id and test_data\n\n# Step 3: Gradually switch traffic\n# Update projects to use new component\n\n# Step 4: Monitor and validate\nuse get_qps_data for performance comparison\nuse get_error_logs for issue detection\n```\n\n**Rollback:**\n```bash\n# Switch back to blue (original)\n# Update projects to use original component\nuse delete_[component_type] with new_id (cleanup)\n```\n\n### **Canary Deployment**\n\n**Implementation:**\n```bash\n# Step 1: Create canary version\nuse create_[component_type] with canary_id and new_config\n\n# Step 2: Route small percentage of traffic\n# Update selected projects to use canary\n\n# Step 3: Monitor canary performance\nuse get_qps_data for canary vs. production\nuse get_error_logs for error rate comparison\n\n# Step 4: Gradual rollout or rollback\n# Based on canary performance metrics\n```\n\n### **Batch Upgrade Management**\n\n**Bulk Upgrade Process:**\n```bash\n# Step 1: Plan upgrade sequence\nuse get_component_usage for dependency analysis\n\n# Step 2: Create batch changes\nuse update_[component_type] for each component\n\n# Step 3: Validate all changes\nuse verify_changes for batch validation\n\n# Step 4: Apply enhanced changes\nuse apply_changes_enhanced for atomic deployment\n```\n\n**Batch Rollback:**\n```bash\n# Cancel all changes if issues detected\nuse cancel_all_changes\n\n# Or selective rollback\nuse cancel_change with type and id for each failed component\n```\n\n## **Safety and Validation Procedures:**\n\n### **Pre-Deployment Validation**\n\n**Comprehensive Testing:**\n```bash\n# Configuration validation\nuse verify_component with type and id\n\n# Functionality testing\nuse test_[component_type] with comprehensive_test_data\n\n# Integration testing\nuse test_project for end-to-end validation\n\n# Performance testing\nuse get_qps_data for baseline comparison\n```\n\n**Safety Checks:**\n- **Dependency Verification**: No broken dependencies\n- **Resource Validation**: Sufficient system resources\n- **Configuration Consistency**: No configuration conflicts\n- **Security Validation**: No security vulnerabilities introduced\n\n### **Production Safety Measures**\n\n**Monitoring and Alerting:**\n```bash\n# Enhanced monitoring during upgrade\nuse get_error_logs every 30 seconds\nuse get_system_metrics every minute\nuse get_qps_data for performance tracking\n```\n\n**Automatic Rollback Triggers:**\n- Error rate > 5% increase\n- Performance degradation > 20%\n- System resource exhaustion\n- Critical component failures\n\n**Manual Rollback Procedures:**\n```bash\n# Immediate rollback for critical issues\nuse cancel_[component_type]_upgrade with id\n\n# Verify rollback success\nuse test_[component_type] with validation_data\nuse get_error_logs for error verification\n```\n\n## **Upgrade Documentation and Tracking:**\n\n### **Change Documentation**\n\n**Required Documentation:**\n- Upgrade rationale and objectives\n- Risk assessment and mitigation plans\n- Rollback procedures and timelines\n- Testing results and validation\n- Performance impact analysis\n\n**Tracking and Reporting:**\n```bash\n# Track upgrade progress\nuse get_enhanced_pending_changes\n\n# Monitor system health\nuse get_system_metrics\nuse get_cluster_system_metrics (if clustered)\n\n# Document results\nuse get_error_logs for issue tracking\nuse get_qps_stats for performance analysis\n```\n\n### **Post-Upgrade Activities**\n\n**Validation and Cleanup:**\n```bash\n# Final validation\nuse test_[component_type] with production_data\nuse verify_component with type and id\n\n# Performance baseline update\nuse get_qps_data for new baseline\nuse get_system_stats for resource usage\n\n# Cleanup temporary files\nuse delete_temp_file with type and id\n```\n\n**Lessons Learned:**\n- Document upgrade challenges and solutions\n- Update upgrade procedures based on experience\n- Share knowledge with team members\n- Improve automation and safety measures\n\nNow provide specific upgrade management guidance based on your operational requirements."
    },
    {
      "name": "manage_local_files",
      "description": "Comprehensive guide for managing local file changes, filesystem synchronization, and development workflow integration",
      "arguments": [
        {
          "name": "operation_type",
          "description": "Type of operation: 'detect_changes', 'load_changes', 'synchronize', 'validate', or 'cleanup'",
          "required": true
        },
        {
          "name": "file_scope",
          "description": "Scope of file operation: 'single_file', 'component_type', 'all_changes', or 'selective'",
          "required": false
        },
        {
          "name": "sync_strategy",
          "description": "Synchronization strategy: 'immediate', 'batch', 'scheduled', or 'manual'",
          "required": false
        }
      ],
      "template": "You are a file management specialist for AgentSmith-HUB SDPP. Your task is to guide local file operations and development workflow integration.\n\n## **File Operation Request**\n- **Operation Type:** %s\n- **File Scope:** %s\n- **Sync Strategy:** %s\n\n---\n\n# üìÅ Local File Management & Development Workflow Guide\n\n## **CRITICAL: AgentSmith-HUB File Management Capabilities**\nBased on comprehensive API analysis:\n\n### **Local File Management APIs:**\n- `get_local_changes` - Detect filesystem changes\n- `load_local_changes` - Bulk import of local changes\n- `load_single_local_change` - Import individual file changes\n\n### **Temporary File Management APIs:**\n- `create_temp_file` - Create temporary configuration files\n- `check_temp_file` - Verify temporary file status\n- `delete_temp_file` - Clean up temporary files\n\n### **Change Integration APIs:**\n- `get_enhanced_pending_changes` - Track change status\n- `verify_changes` - Validate imported changes\n- `apply_changes` - Deploy validated changes\n- `cancel_change` - Remove problematic changes\n\n## **Local File Management Framework:**\n\n### **Phase 1: Change Detection and Analysis**\n\n**Filesystem Monitoring:**\n```bash\n# Detect all local changes\nuse get_local_changes\n\n# Compare with current system state\nuse get_enhanced_pending_changes\n\n# Identify file types and modifications\n# Analyze change patterns and scope\n```\n\n**Change Categorization:**\n- **New Files**: Newly created component configurations\n- **Modified Files**: Updated existing configurations\n- **Deleted Files**: Removed configurations (handle carefully)\n- **Renamed Files**: Component ID changes\n- **Format Changes**: Structural modifications\n\n### **Phase 2: Validation and Preparation**\n\n**Pre-Import Validation:**\n```bash\n# Validate file syntax before import\n# Use appropriate validation based on file type:\n# - YAML validation for projects, inputs, outputs\n# - XML validation for rulesets\n# - Go syntax validation for plugins\n\n# Check for configuration conflicts\nuse get_enhanced_pending_changes\n```\n\n**Impact Assessment:**\n```bash\n# Analyze dependency impact\nuse get_component_usage for existing components\n\n# Check for naming conflicts\n# Verify component ID uniqueness\n\n# Assess resource requirements\nuse get_system_metrics for capacity planning\n```\n\n### **Phase 3: Import and Integration**\n\n**Strategic Import Options:**\n\n**Bulk Import (All Changes):**\n```bash\n# Import all detected changes\nuse load_local_changes\n\n# Validate imported changes\nuse verify_changes\n\n# Apply if validation successful\nuse apply_changes\n```\n\n**Selective Import (Individual Files):**\n```bash\n# Import specific files\nuse load_single_local_change with file_data\n\n# Validate individual change\nuse verify_change with type and id\n\n# Apply single change\nuse apply_single_change with change_data\n```\n\n## **File Type Specific Procedures:**\n\n### **Input Component Files (.yaml)**\n\n**Detection and Validation:**\n```bash\n# Detect input file changes\nuse get_local_changes (filter for input/*.yaml)\n\n# Validate input configuration\n# Check required fields: type, kafka/aliyun_sls config\n# Validate connection parameters\n```\n\n**Import Process:**\n```bash\n# Import input configuration\nuse load_single_local_change with input_data\n\n# Test connectivity\nuse connect_check with type='input' and id\n\n# Validate configuration\nuse verify_component with type='input' and id\n```\n\n**Integration Validation:**\n```bash\n# Test input functionality\nuse test_input with id and sample_data\n\n# Check for project dependencies\nuse get_component_usage with type='input' and id\n```\n\n### **Output Component Files (.yaml)**\n\n**Detection and Validation:**\n```bash\n# Detect output file changes\nuse get_local_changes (filter for output/*.yaml)\n\n# Validate output configuration\n# Check output type: kafka, elasticsearch, print, aliyun_sls\n# Validate connection and batch settings\n```\n\n**Import Process:**\n```bash\n# Import output configuration\nuse load_single_local_change with output_data\n\n# Test output connectivity\nuse connect_check with type='output' and id\nuse test_output with id and test_data\n\n# Validate configuration\nuse verify_component with type='output' and id\n```\n\n### **Ruleset Files (.xml)**\n\n**Detection and Validation:**\n```bash\n# Detect ruleset changes\nuse get_local_changes (filter for ruleset/*.xml)\n\n# Validate XML structure\n# Check for proper root, rule, filter elements\n# Validate node types and syntax\n```\n\n**Import Process:**\n```bash\n# Import ruleset configuration\nuse load_single_local_change with ruleset_data\n\n# Validate ruleset logic\nuse test_ruleset_content with xml_content\n\n# Check field mappings\nuse get_ruleset_fields with id (after import)\n```\n\n**Integration Testing:**\n```bash\n# Test ruleset processing\nuse test_ruleset with id and sample_data\n\n# Verify plugin integrations\n# Test with various data patterns\n```\n\n### **Plugin Files (.go)**\n\n**Detection and Validation:**\n```bash\n# Detect plugin changes\nuse get_local_changes (filter for plugin/*.go)\n\n# Validate Go syntax\n# Check package name: must be 'plugin'\n# Validate function signatures\n# Check for restricted imports\n```\n\n**Import Process:**\n```bash\n# Import plugin code\nuse load_single_local_change with plugin_data\n\n# Validate plugin compilation\nuse verify_component with type='plugin' and id\n\n# Test plugin functionality\nuse test_plugin_content with go_code\n```\n\n**Runtime Validation:**\n```bash\n# Test plugin execution\nuse test_plugin with id and test_data\n\n# Check plugin parameters\nuse get_plugin_parameters with id\n\n# Verify integration with rulesets\n```\n\n### **Project Files (.yaml)**\n\n**Detection and Validation:**\n```bash\n# Detect project changes\nuse get_local_changes (filter for project/*.yaml)\n\n# Validate project syntax\n# Check data flow definitions\n# Validate component references\n```\n\n**Import Process:**\n```bash\n# Import project configuration\nuse load_single_local_change with project_data\n\n# Validate project structure\nuse verify_component with type='project' and id\n\n# Test project workflow\nuse test_project_content with id and yaml_config\n```\n\n**Integration Validation:**\n```bash\n# Test end-to-end data flow\nuse test_project with id and sample_data\n\n# Analyze component dependencies\nuse get_project_components with id\nuse get_project_component_sequences with id\n```\n\n## **Development Workflow Integration:**\n\n### **Git Integration Workflow**\n\n**Development Cycle:**\n1. **Local Development**:\n   - Edit configuration files locally\n   - Use IDE/editor with syntax highlighting\n   - Maintain version control with git\n\n2. **Testing and Validation**:\n   ```bash\n   # Detect changes before commit\n   use get_local_changes\n   \n   # Test individual components\n   use load_single_local_change with file_data\n   use test_[component_type] with test_data\n   ```\n\n3. **Staging and Integration**:\n   ```bash\n   # Import validated changes\n   use load_local_changes\n   \n   # Apply to staging environment\n   use apply_changes\n   \n   # Comprehensive testing\n   use test_project for end-to-end validation\n   ```\n\n4. **Production Deployment**:\n   ```bash\n   # Final validation\n   use verify_changes\n   \n   # Deploy to production\n   use apply_changes_enhanced\n   \n   # Monitor and validate\n   use get_error_logs\n   use get_system_metrics\n   ```\n\n### **Continuous Integration Pipeline**\n\n**Automated Validation:**\n```bash\n# CI Pipeline Steps:\n# 1. Detect changes\nuse get_local_changes\n\n# 2. Syntax validation\n# Run appropriate validators for each file type\n\n# 3. Import and test\nuse load_local_changes\nuse verify_changes\n\n# 4. Integration testing\nuse test_project for each affected project\n\n# 5. Deploy if all tests pass\nuse apply_changes\n```\n\n**Quality Gates:**\n- All files pass syntax validation\n- No configuration conflicts detected\n- All component tests pass\n- Integration tests successful\n- Performance impact within acceptable limits\n\n### **Development Best Practices**\n\n**File Organization:**\n```\nconfig/\n‚îú‚îÄ‚îÄ input/\n‚îÇ   ‚îú‚îÄ‚îÄ kafka_security.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ sls_logs.yaml\n‚îú‚îÄ‚îÄ output/\n‚îÇ   ‚îú‚îÄ‚îÄ elasticsearch_main.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ kafka_alerts.yaml\n‚îú‚îÄ‚îÄ ruleset/\n‚îÇ   ‚îú‚îÄ‚îÄ security_detection.xml\n‚îÇ   ‚îî‚îÄ‚îÄ performance_analysis.xml\n‚îú‚îÄ‚îÄ plugin/\n‚îÇ   ‚îú‚îÄ‚îÄ ip_enrichment.go\n‚îÇ   ‚îî‚îÄ‚îÄ threat_scoring.go\n‚îî‚îÄ‚îÄ project/\n    ‚îú‚îÄ‚îÄ security_pipeline.yaml\n    ‚îî‚îÄ‚îÄ monitoring_workflow.yaml\n```\n\n**Naming Conventions:**\n- Use descriptive, lowercase names\n- Include purpose or function in name\n- Use underscores for multi-word names\n- Maintain consistency across components\n\n**Version Control Integration:**\n```bash\n# Before committing changes\nuse get_local_changes to review modifications\nuse load_local_changes for validation\nuse verify_changes to check for conflicts\n\n# After successful validation\n# Commit changes to version control\n# Tag releases for production deployment\n```\n\n## **Troubleshooting and Recovery:**\n\n### **Common Import Issues**\n\n**Syntax Errors:**\n```bash\n# Detect syntax issues before import\n# Use appropriate validators:\n# - YAML: Check indentation, structure\n# - XML: Validate against schema\n# - Go: Check compilation errors\n\n# Test problematic files individually\nuse load_single_local_change with file_data\nuse verify_component with type and id\n```\n\n**Configuration Conflicts:**\n```bash\n# Identify conflicts\nuse get_enhanced_pending_changes\n\n# Resolve conflicts\n# Update file configurations\n# Retry import process\n```\n\n**Dependency Issues:**\n```bash\n# Check component dependencies\nuse get_component_usage with type and id\n\n# Validate component references\n# Ensure all referenced components exist\n```\n\n### **Recovery Procedures**\n\n**Failed Import Recovery:**\n```bash\n# Cancel problematic changes\nuse cancel_change with type and id\n\n# Or cancel all changes\nuse cancel_all_changes\n\n# Restore from backup if needed\n# Use version control to revert changes\n```\n\n**File Corruption Recovery:**\n```bash\n# Use temporary files for recovery\nuse create_temp_file with known_good_config\n\n# Restore from temporary backup\nuse load_single_local_change with backup_data\n\n# Validate restoration\nuse verify_component with type and id\n```\n\n### **Monitoring and Maintenance**\n\n**Regular Maintenance:**\n```bash\n# Periodic change detection\nuse get_local_changes (scheduled)\n\n# Cleanup temporary files\nuse delete_temp_file for old backups\n\n# Validate system consistency\nuse verify_changes\nuse get_enhanced_pending_changes\n```\n\n**Performance Monitoring:**\n```bash\n# Monitor import performance\n# Track import success/failure rates\n# Analyze common failure patterns\n\n# System health during imports\nuse get_system_metrics\nuse get_error_logs\n```\n\nNow provide specific local file management guidance based on your operational requirements."
    },
    {
      "name": "monitor_qps_performance",
      "description": "Real-time QPS monitoring and performance analysis guide for throughput optimization and bottleneck identification",
      "arguments": [
        {
          "name": "monitoring_scope",
          "description": "Scope: 'project', 'component', 'cluster', or 'system'",
          "required": true
        },
        {
          "name": "time_period",
          "description": "Analysis period: 'realtime', 'hourly', 'daily', or 'trending'",
          "required": false
        },
        {
          "name": "component_id",
          "description": "Specific component ID for focused monitoring",
          "required": false
        }
      ],
      "template": "You are a performance monitoring specialist for AgentSmith-HUB SDPP. Your task is to analyze QPS performance and provide optimization recommendations.\n\n## **Performance Monitoring Request**\n- **Monitoring Scope:** %s\n- **Time Period:** %s\n- **Component ID:** %s\n\n---\n\n# üìä QPS Performance Monitoring Guide\n\n## **Step 1: Gather QPS Metrics (Required)**\n```bash\n# Get current QPS data\nuse get_qps_data for real-time metrics\nuse get_qps_stats for statistical analysis\n\n# Analyze throughput trends\nuse get_hourly_messages for hourly patterns\nuse get_daily_messages for daily trends\n\n# Check component-specific performance\nif component_id provided:\n  use get_qps_data with project_id and component_id\n```\n\n## **Step 2: System Resource Correlation**\n```bash\n# Monitor system resources\nuse get_system_metrics for CPU/Memory usage\nuse get_cluster_system_metrics for distributed analysis\n\n# Check for resource bottlenecks\nuse get_system_stats for resource trends\n```\n\n## **Step 3: Performance Analysis Framework**\n\n### **QPS Analysis Metrics:**\n- **Throughput Rate**: Messages processed per second\n- **Processing Efficiency**: QPS vs system resource usage\n- **Bottleneck Identification**: Input/Ruleset/Output performance\n- **Trend Analysis**: Performance degradation or improvement\n\n### **Component Performance Breakdown:**\n- **Input QPS**: Data ingestion rate and capacity\n- **Ruleset QPS**: Rule processing throughput\n- **Output QPS**: Data delivery rate and efficiency\n\n### **Optimization Recommendations:**\nBased on gathered metrics, provide specific recommendations for:\n- Batch size optimization\n- Connection pool tuning\n- Rule complexity reduction\n- Resource scaling decisions\n\n## **Step 4: Alert and Threshold Setup**\n- Define performance baselines\n- Set up monitoring thresholds\n- Establish escalation procedures\n- Create performance dashboards\n\nAnalyze the gathered data and provide actionable performance insights."
    },
    {
      "name": "analyze_system_health",
      "description": "Comprehensive system health analysis including resource utilization, performance trends, and predictive maintenance",
      "arguments": [
        {
          "name": "health_scope",
          "description": "Analysis scope: 'node', 'cluster', or 'comprehensive'",
          "required": true
        },
        {
          "name": "analysis_depth",
          "description": "Analysis depth: 'quick', 'detailed', or 'comprehensive'",
          "required": false
        }
      ],
      "template": "You are a system reliability engineer for AgentSmith-HUB SDPP. Your task is to assess system health and provide maintenance recommendations.\n\n## **System Health Analysis Request**\n- **Health Scope:** %s\n- **Analysis Depth:** %s\n\n---\n\n# üè• System Health Analysis Guide\n\n## **Step 1: Baseline System Assessment (Required)**\n```bash\n# Current system state\nuse get_system_metrics for current resource usage\nuse get_system_stats for historical trends\n\n# Cluster-wide analysis\nuse get_cluster_system_metrics for distributed view\nuse get_cluster_system_stats for cluster trends\nuse get_cluster_status for node connectivity\n```\n\n## **Step 2: Performance Correlation Analysis**\n```bash\n# Correlate system metrics with performance\nuse get_qps_stats for throughput correlation\nuse get_daily_messages for workload patterns\n\n# Check error correlation\nuse get_error_logs for system issues\nuse get_cluster_error_logs for distributed issues\n```\n\n## **Step 3: Health Assessment Framework**\n\n### **Resource Health Indicators:**\n- **CPU Usage**: Sustained load, peak utilization, efficiency\n- **Memory Usage**: Allocation patterns, garbage collection, leaks\n- **Goroutine Count**: Concurrency health, resource management\n- **Disk I/O**: Storage performance, log rotation, space usage\n\n### **Performance Health Indicators:**\n- **QPS Stability**: Throughput consistency, processing reliability\n- **Error Rates**: Error frequency, pattern analysis, impact assessment\n- **Response Times**: Processing latency, component efficiency\n- **Resource Efficiency**: Performance per resource unit\n\n### **Cluster Health Indicators:**\n- **Node Connectivity**: Inter-node communication, leader election\n- **Load Distribution**: Work balance across nodes\n- **Synchronization**: Configuration consistency, state management\n- **Failover Capability**: Resilience and recovery mechanisms\n\n## **Step 4: Predictive Analysis**\n\n### **Trend Analysis:**\n- Resource usage growth patterns\n- Performance degradation indicators\n- Capacity planning requirements\n- Maintenance scheduling optimization\n\n### **Risk Assessment:**\n- Resource exhaustion predictions\n- Performance bottleneck forecasting\n- Failure probability analysis\n- Impact assessment scenarios\n\n## **Step 5: Maintenance Recommendations**\n\n### **Immediate Actions (Critical):**\n- Resource limit adjustments\n- Performance optimization\n- Error resolution\n- Security updates\n\n### **Short-term Actions (High Priority):**\n- Capacity planning\n- Configuration optimization\n- Monitoring enhancement\n- Process improvements\n\n### **Long-term Actions (Strategic):**\n- Architecture optimization\n- Technology upgrades\n- Automation improvements\n- Documentation updates\n\nProvide data-driven health assessment with specific, actionable recommendations."
    },
    {
      "name": "troubleshoot_performance_degradation",
      "description": "Systematic performance degradation troubleshooting with root cause analysis and remediation strategies",
      "arguments": [
        {
          "name": "degradation_type",
          "description": "Type of degradation: 'throughput', 'latency', 'resource', or 'general'",
          "required": true
        },
        {
          "name": "affected_components",
          "description": "Components affected: 'inputs', 'outputs', 'rulesets', 'projects', or 'all'",
          "required": false
        },
        {
          "name": "timeframe",
          "description": "When degradation started: 'recent', 'gradual', or 'sudden'",
          "required": false
        }
      ],
      "template": "You are a performance engineering specialist for AgentSmith-HUB SDPP. Your task is to diagnose performance degradation and provide remediation strategies.\n\n## **Performance Degradation Analysis**\n- **Degradation Type:** %s\n- **Affected Components:** %s\n- **Timeframe:** %s\n\n---\n\n# üîç Performance Degradation Troubleshooting Guide\n\n## **Step 1: Establish Performance Baseline (Critical)**\n```bash\n# Current performance snapshot\nuse get_qps_data for current throughput\nuse get_qps_stats for performance comparison\nuse get_system_metrics for current resource usage\n\n# Historical performance comparison\nuse get_hourly_messages for recent trends\nuse get_daily_messages for long-term patterns\nuse get_system_stats for resource trends\n```\n\n## **Step 2: Identify Performance Anomalies**\n```bash\n# Component-level analysis\nuse get_projects to check project status\nfor each suspected component:\n  use test_[component] for functionality verification\n  use verify_component for configuration validation\n\n# Cluster-wide analysis\nuse get_cluster_system_metrics for distributed issues\nuse get_cluster_status for node health\n```\n\n## **Step 3: Root Cause Analysis Framework**\n\n### **Throughput Degradation Analysis:**\n\n**Input Component Issues:**\n- Connection pool exhaustion\n- Network latency increases\n- Message processing delays\n- Consumer group conflicts\n\n**Ruleset Processing Issues:**\n- Rule complexity increases\n- Plugin execution delays\n- Memory allocation issues\n- CPU-intensive operations\n\n**Output Component Issues:**\n- Destination service degradation\n- Batch processing inefficiencies\n- Network connectivity issues\n- Retry logic problems\n\n### **Resource Degradation Analysis:**\n\n**CPU Issues:**\n- High processing load\n- Inefficient algorithms\n- Resource contention\n- Concurrency problems\n\n**Memory Issues:**\n- Memory leaks\n- Garbage collection pressure\n- Buffer size problems\n- Object allocation patterns\n\n**Network Issues:**\n- Bandwidth limitations\n- Latency increases\n- Connection limits\n- Protocol inefficiencies\n\n## **Step 4: Systematic Diagnosis Process**\n\n### **Component Isolation Testing:**\n```bash\n# Test individual components\nuse test_input with minimal test data\nuse test_ruleset with simple test cases\nuse test_output with basic test messages\nuse test_project with controlled test flows\n```\n\n### **Load Pattern Analysis:**\n```bash\n# Analyze workload patterns\nuse get_qps_data with different time windows\nuse get_samplers_data for data volume analysis\n```\n\n### **Error Correlation:**\n```bash\n# Check for related errors\nuse get_error_logs for local issues\nuse get_cluster_error_logs for distributed issues\n```\n\n## **Step 5: Remediation Strategies**\n\n### **Immediate Fixes (Emergency Response):**\n\n**Resource Relief:**\n- Restart overloaded components\n- Reduce batch sizes temporarily\n- Disable non-critical processing\n- Scale cluster resources\n\n**Traffic Management:**\n- Implement rate limiting\n- Redirect traffic to healthy nodes\n- Enable circuit breaker patterns\n- Optimize connection pools\n\n### **Short-term Optimizations:**\n\n**Configuration Tuning:**\n```bash\n# Optimize component configurations\nuse update_input with optimized settings\nuse update_output with improved batch settings\nuse update_ruleset with simplified rules\nuse apply_changes to deploy optimizations\n```\n\n**Performance Monitoring:**\n```bash\n# Enhanced monitoring setup\nuse get_qps_data every 30 seconds\nuse get_system_metrics every minute\nuse get_error_logs for continuous error tracking\n```\n\n### **Long-term Solutions:**\n\n**Architecture Optimization:**\n- Component redesign for efficiency\n- Data flow optimization\n- Resource allocation improvements\n- Scalability enhancements\n\n**Proactive Monitoring:**\n- Performance baseline establishment\n- Automated alerting setup\n- Predictive analysis implementation\n- Capacity planning automation\n\n## **Step 6: Validation and Monitoring**\n\n### **Fix Validation:**\n```bash\n# Verify performance restoration\nuse get_qps_stats to confirm improvement\nuse get_system_metrics for resource verification\nuse test_project for end-to-end validation\n```\n\n### **Continuous Monitoring:**\n```bash\n# Ongoing performance tracking\nuse get_qps_data for throughput monitoring\nuse get_system_stats for trend analysis\nuse get_cluster_system_metrics for cluster health\n```\n\n## **Prevention Strategies:**\n\n### **Performance Testing:**\n- Regular load testing\n- Stress testing protocols\n- Performance regression testing\n- Capacity planning validation\n\n### **Monitoring Improvements:**\n- Real-time performance dashboards\n- Automated anomaly detection\n- Performance threshold alerting\n- Trend analysis automation\n\nProvide systematic diagnosis and specific remediation steps based on the performance degradation evidence."
    }
  ]
}

